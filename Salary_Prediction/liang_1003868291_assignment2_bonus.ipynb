{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is the bonus part. In order to have an interactive environment to discuss the model performance, I got the permission from TA and submitted an ipynb file instead of python file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install keras\n",
    "!pip install seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import make_scorer, r2_score, mean_squared_error, auc, mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP1  Import data and do featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanwen/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (1,3,9,11,13,22,24,25,26,27,28,29,45,57,65,84,86,88,108,110,124,126,151,195,209,224,250,263,265,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,305,307,323,326,327,330,342,372,385,386,394,395) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "Salary = pd.read_csv('./Kaggle_Salary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13995, 605) (9796, 605)\n"
     ]
    }
   ],
   "source": [
    "# Drop three irrelavent columns\n",
    "Salary.drop(['Unnamed: 0','index'], axis=1,inplace=True)\n",
    "\n",
    "QuestionMapping = {}\n",
    "for col in Salary.columns:\n",
    "    QuestionMapping[col] = Salary.loc[0,col]\n",
    "Salary.drop([0],inplace=True)\n",
    "# print(Salary.shape)\n",
    "Salary.head()\n",
    "\n",
    "# See how many null values are in each column\n",
    "# Here we only deal with those colunmns that has null values more than zero but fewer that 1% of all data\n",
    "less_001 = []\n",
    "for col in Salary.columns:\n",
    "    NullNum = Salary[col].isnull().sum()\n",
    "    if (NullNum > 0) & (0.01 * Salary.shape[0] > NullNum):\n",
    "        if (\"Part\" not in col) or (col in ['Q39_Part_1','Q39_Part_2']):\n",
    "            less_001.append(col)\n",
    "# print(\"We drop nan rows in the below columns:\",less_001)\n",
    "\n",
    "for col in less_001:\n",
    "    Salary.dropna(subset=[col],inplace=True)\n",
    "# print(Salary.shape)\n",
    "between_001_020 = []\n",
    "for col in Salary.columns:\n",
    "    NullNum = Salary[col].isnull().sum()\n",
    "    if (0.2 * Salary.shape[0] > NullNum) and (NullNum >= 0.01 * Salary.shape[0]):\n",
    "        if (\"Part\" not in col) or (col in ['Q39_Part_1','Q39_Part_2']):\n",
    "            between_001_020.append(col)\n",
    "# print(\"We use modes in these columns to replace nan value:\",between_001_020)\n",
    "for col in between_001_020:\n",
    "    Salary[col].fillna(Salary[col].mode()[0],inplace=True)\n",
    "\n",
    "between_020_050 = []\n",
    "for col in Salary.columns:\n",
    "    NullNum = Salary[col].isnull().sum()\n",
    "    if (0.2 * Salary.shape[0]) < NullNum and (NullNum < 0.5 * Salary.shape[0]):\n",
    "        if (\"Part\" not in col) or (col in ['Q39_Part_1','Q39_Part_2']):\n",
    "            between_020_050.append(col)\n",
    "# print(\"We set nans in these columns as 'Unknown':\",between_020_050)\n",
    "\n",
    "for col in between_020_050:\n",
    "    Salary.loc[Salary[col].isnull(),col] = 'Unknown'\n",
    "more_050 = []\n",
    "for col in Salary.columns:\n",
    "    NullNum = Salary[col].isnull().sum()\n",
    "    if 0.5 * Salary.shape[0] < NullNum:\n",
    "        if (\"Part\" not in col) or (col in ['Q39_Part_1','Q39_Part_2']):\n",
    "            more_050.append(col)\n",
    "# print(\"We drop these columns because large part of the data is missing:\",more_050)\n",
    "\n",
    "for col in more_050:\n",
    "    Salary.drop([col], axis=1,inplace=True)\n",
    "ALLCOL = Salary.columns.tolist()\n",
    "def select_col(condition, allcol=ALLCOL):\n",
    "    digit = len(condition)\n",
    "    selected = []\n",
    "    for i in allcol:\n",
    "        if condition in i:\n",
    "            if len(i)==digit:\n",
    "                selected.append(i)\n",
    "            elif i[digit] not in '0123456789':\n",
    "                selected.append(i)                \n",
    "    return selected\n",
    "DummyCols = ['Q1','Q4','Q5','Q6','Q7','Q10','Q12_MULTIPLE_CHOICE',\\\n",
    "            'Q17','Q18','Q20','Q23','Q26','Q32','Q39_Part_1', 'Q39_Part_2',\\\n",
    "             'Q40','Q48']\n",
    "for col in DummyCols:\n",
    "    Salary = pd.get_dummies(data=Salary, columns=[col])\n",
    "# print(Salary['Q3'].value_counts())\n",
    "val_counts = Salary['Q3'].value_counts()\n",
    "value_mask = Salary['Q3'].isin(val_counts.index[val_counts < 50])\n",
    "Salary.loc[value_mask,'Q3'] = \"Other\"\n",
    "\n",
    "Salary = pd.get_dummies(data=Salary, columns=['Q3'])\n",
    "# print(Salary['Q22'].value_counts())\n",
    "val_counts = Salary['Q22'].value_counts()\n",
    "value_mask = Salary['Q22'].isin(val_counts.index[val_counts < 100])\n",
    "Salary.loc[value_mask,'Q22'] = \"Other\"\n",
    "\n",
    "Salary = pd.get_dummies(data=Salary, columns=['Q22'])\n",
    "def Range_Normalize(value):\n",
    "    if isinstance(value, str):\n",
    "        if '-' in value:\n",
    "            temp = value.split('-')\n",
    "            return (float(temp[0])+float(temp[1]))/2\n",
    "        if '+' in value:\n",
    "            a = value.split('+')\n",
    "            return float(a[0])\n",
    "        else:\n",
    "            return value\n",
    "    else:\n",
    "        return value\n",
    "# Salary[select_col('Q2')]\n",
    "Salary['Q2_NORMAL'] = Salary['Q2'].apply(Range_Normalize)\n",
    "Salary.drop(['Q2'],axis=1,inplace=True)\n",
    "# Salary[select_col('Q8')]\n",
    "Salary['Q8_NORMAL'] = Salary['Q8'].apply(Range_Normalize)\n",
    "Salary.drop(['Q8'],axis=1,inplace=True)\n",
    "# Salary[select_col('Q24')]\n",
    "def Q24Range_Normalize(value):\n",
    "    mapdict = {'I have never written code and I do not want to learn':-1,\n",
    "               'I have never written code but I want to learn':0,'< 1 year':0.5,\n",
    "              '1-2 years':1.5,'3-5 years':4,'5-10 years':7.5,'10-20 years':15,\n",
    "               '20-30 years':25,'30-40 years':35,'40+ years':40}\n",
    "    return mapdict[value]\n",
    "# Salary[select_col('Q2')]\n",
    "Salary['Q24_NORMAL'] = Salary['Q24'].apply(Q24Range_Normalize)\n",
    "Salary.drop(['Q24'],axis=1,inplace=True)\n",
    "# Salary[select_col('Q25')]\n",
    "Salary['Q25'].unique()\n",
    "def Q25Range_Normalize(value):\n",
    "    mapdict = {'I have never studied machine learning and I do not plan to':-1,\n",
    "               'I have never studied machine learning but plan to learn in the future':0,\n",
    "               '< 1 year':0.5,'1-2 years':1.5,'2-3 years':2.5,'3-4 years':3.5,'4-5 years':4.5,\n",
    "               '5-10 years':7.5,'10-15 years':12.5,'20+ years':20}\n",
    "    return mapdict[value]\n",
    "\n",
    "Salary['Q25_NORMAL'] = Salary['Q25'].apply(Q25Range_Normalize)\n",
    "Salary.drop(['Q25'],axis=1,inplace=True)\n",
    "def Q41Range_Normalize(value):\n",
    "    mapdict = {'Very important':10,\n",
    "               'Slightly important':5,\n",
    "               'No opinion; I do not know':2,\n",
    "               'Unknown':0,\n",
    "               'Not at all important':-10}\n",
    "    return mapdict[value]\n",
    "\n",
    "Salary.loc[Salary['Q41_Part_1'].isnull(),'Q41_Part_1'] = 'Unknown'\n",
    "Salary.loc[Salary['Q41_Part_2'].isnull(),'Q41_Part_2'] = 'Unknown'\n",
    "Salary.loc[Salary['Q41_Part_3'].isnull(),'Q41_Part_3'] = 'Unknown'\n",
    "\n",
    "Salary['Q41_Part_1_NORMAL'] = Salary['Q41_Part_1'].apply(Q41Range_Normalize)\n",
    "Salary.drop(['Q41_Part_1'],axis=1,inplace=True)\n",
    "Salary['Q41_Part_2_NORMAL'] = Salary['Q41_Part_2'].apply(Q41Range_Normalize)\n",
    "Salary.drop(['Q41_Part_2'],axis=1,inplace=True)\n",
    "Salary['Q41_Part_3_NORMAL'] = Salary['Q41_Part_3'].apply(Q41Range_Normalize)\n",
    "Salary.drop(['Q41_Part_3'],axis=1,inplace=True)\n",
    "def Q43Range_Normalize(value):\n",
    "    if isinstance(value, str):\n",
    "        if '-' in value:\n",
    "            temp = value.split('-')\n",
    "            return (float(temp[0])+float(temp[1]))/2\n",
    "        elif value == \"0\":\n",
    "            return 0\n",
    "        else:\n",
    "            return float(\"nan\")\n",
    "    else:\n",
    "        return value\n",
    "Salary['Q43_NORMAL'] = Salary['Q43'].apply(Q43Range_Normalize)\n",
    "Salary.drop(['Q43'],axis=1,inplace=True)\n",
    "Salary['Q43_NORMAL'].fillna(Salary['Q43_NORMAL'].mode()[0],inplace=True)\n",
    "\n",
    "Salary['Q46_NORMAL'] = Salary['Q46'].apply(Q43Range_Normalize)\n",
    "Salary.drop(['Q46'],axis=1,inplace=True)\n",
    "Salary['Q46_NORMAL'].fillna(Salary['Q46_NORMAL'].mode()[0],inplace=True)\n",
    "# Salary[select_col('Q11')]\n",
    "import math\n",
    "def Binarize(value):\n",
    "    if isinstance(value,str):\n",
    "        return 1\n",
    "    else:\n",
    "        if value == 0 or math.isnan(value):\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "def binarize_col(columns, data=Salary):\n",
    "    for col in columns:\n",
    "        data[col] = data[col].apply(Binarize)\n",
    "NormalCols = select_col('Q11_Part') + select_col('Q13_Part') + \\\n",
    "select_col('Q14_Part') + select_col('Q15_Part') + select_col('Q16_Part')\\\n",
    "+ select_col('Q19_Part') + select_col('Q21_Part') + select_col('Q27_Part')\\\n",
    "+ select_col('Q28_Part') + select_col('Q29_Part') + select_col('Q30_Part') +\\\n",
    "select_col('Q31_Part') + select_col('Q33_Part') + select_col('Q34_Part') +\\\n",
    "select_col('Q36_Part') + select_col('Q38_Part') + select_col('Q42_Part') + \\\n",
    "select_col('Q44_Part') + select_col('Q45_Part') + select_col('Q47_Part') + \\\n",
    "select_col('Q49_Part') + select_col('Q50_Part')\n",
    "binarize_col(NormalCols)\n",
    "def TBinarize(value):\n",
    "    if isinstance(value,str):\n",
    "        return float(value)\n",
    "    else:\n",
    "        if math.isnan(value):\n",
    "            return -1\n",
    "        else:\n",
    "            return value\n",
    "def Tbinarize_col(columns, data=Salary):\n",
    "    for col in columns:\n",
    "        data[col] = data[col].apply(TBinarize)\n",
    "\n",
    "Tbinarize_col(select_col('Q34_Part'))\n",
    "Tbinarize_col(select_col('Q35_Part'))\n",
    "for col in Salary.columns:\n",
    "    if isinstance(Salary.loc[1,col],str):\n",
    "        Salary[col] = Salary[col].apply(lambda x:float(x))\n",
    "Salary.rename(columns={\"Q9\": \"Yearly_compensation\"},inplace=True)\n",
    "Salary.rename(columns={\"Time from Start to Finish (seconds)\": \"TimeUse\"},inplace=True)\n",
    "Salary.drop(Salary[Salary.TimeUse < 180].index,inplace=True)\n",
    "for col in Salary.columns:\n",
    "    if len(Salary[col].value_counts())==1:\n",
    "        Salary.drop([col], axis=1,inplace=True)\n",
    "Salary.drop(Salary[Salary.Yearly_compensation < 1000].index, inplace=True)\n",
    "Salary.drop(Salary[Salary.Yearly_compensation > 350000].index, inplace=True)\n",
    "X = Salary.drop(['Yearly_compensation'],axis=1)\n",
    "Y = Salary['Yearly_compensation']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X,Y,test_size=0.3, random_state=20)\n",
    "print(X.shape,X_Train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Neural Network is complex model with much parameters. Here we use all the features after featurization which have a dimension 605. Before feeding to the model, we standardized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9796, 605)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "def feature_scaling(X_Train, X_Test, method):\n",
    "    methodmap = {'z-score':StandardScaler(),'min-max':MinMaxScaler(),'robust':RobustScaler()}\n",
    "    if method not in ['z-score','min-max','robust']:\n",
    "        print(\"We don't have this kind of method for normalization. Please choose from ['z-score','min-max','robust']\")\n",
    "        return pd.DataFrame(X_Train), pd.DataFrame(X_Test)\n",
    "    else:\n",
    "        ss = methodmap[method].fit(X_Train)\n",
    "        X_Train = ss.transform(X_Train)\n",
    "        X_Test = ss.transform(X_Test)\n",
    "        return pd.DataFrame(X_Train), pd.DataFrame(X_Test)\n",
    "X_Train_All, X_Test_All = feature_scaling(X_Train,X_Test,'z-score')\n",
    "print(X_Train_All.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP2  Develop a Baseline Neural Network Model\n",
    "\n",
    "Below we create the baseline model to be evaluated. It is a simple model that has two single fully connected hidden layers with the same number of neurons as input attributes (605). The network uses good practices such as the rectifier activation function for the hidden layer. No activation function is used for the output layer because it is a regression problem and we are interested in predicting numerical values directly without transform. The efficient ADAM optimization algorithm is used and a mean squared error loss function is optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanwen/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Learner(Vanilla/Basic Model)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss(Based on Testing Set): 1141378709.1555133\n",
      "33784.29693975253 22446.168089567524\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "\n",
    "# Stack of Layers\n",
    "model1.add(Dense(units=600, kernel_initializer='normal', activation='relu', input_dim=605))\n",
    "model1.add(Dense(units=500, kernel_initializer='normal', activation='relu'))\n",
    "model1.add(Dense(units=1, kernel_initializer='normal'))\n",
    "\n",
    "model1.compile(loss='mean_squared_error',optimizer='adam') \n",
    "\n",
    "Kmod = model1.fit(X_Train_All,Y_Train, epochs=2000, batch_size=20, verbose=0)\n",
    "Error = model1.evaluate(X_Test_All, Y_Test, batch_size=50, verbose=0)\n",
    "print('\\nLoss(Based on Testing Set):', Error)\n",
    "\n",
    "Y_Test_pred = model1.predict(X_Test_All)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Test,Y_Test_pred))\n",
    "MAE = mean_absolute_error(Y_Test,Y_Test_pred)\n",
    "\n",
    "print(RMSE,MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "823.3228728623982 544.2172392023101\n"
     ]
    }
   ],
   "source": [
    "Y_Train_pred = model1.predict(X_Train_All)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Train,Y_Train_pred))\n",
    "MAE = mean_absolute_error(Y_Train,Y_Train_pred)\n",
    "\n",
    "print(RMSE,MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP3  Tune The Neural Network Topology\n",
    "There are many concerns that can be optimized for a neural network model. The point of biggest leverage maybe the structure of the network itself, including the number of layers and the number of neurons in each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Evaluate a Deeper Network Topology\n",
    "\n",
    "In this section we evaluate two additional network topologies in an effort to further improve the performance of the model. We look at deeper network topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss(Based on Testing Set): 1114005714.4729698\n",
      "33376.72416123762 21768.40713815673\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "# Stack of Layers\n",
    "model2.add(Dense(units=600, kernel_initializer='normal', activation='relu', input_dim=605))\n",
    "model2.add(Dense(units=500, kernel_initializer='normal', activation='relu'))\n",
    "model2.add(Dense(units=400, kernel_initializer='normal', activation='relu'))\n",
    "model2.add(Dense(units=1, kernel_initializer='normal'))\n",
    "\n",
    "model2.compile(loss='mean_squared_error',optimizer='adam') \n",
    "\n",
    "Kmod = model2.fit(X_Train_All,Y_Train, epochs=2000, batch_size=20, verbose=0)\n",
    "Error = model2.evaluate(X_Test_All, Y_Test, batch_size=50, verbose=0)\n",
    "print('\\nLoss(Based on Testing Set):', Error)\n",
    "\n",
    "Y_Test_pred = model2.predict(X_Test_All)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Test,Y_Test_pred))\n",
    "MAE = mean_absolute_error(Y_Test,Y_Test_pred)\n",
    "\n",
    "print(RMSE,MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1140.4459384107058 738.7390327998793\n"
     ]
    }
   ],
   "source": [
    "Y_Train_pred = model2.predict(X_Train_All)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Train,Y_Train_pred))\n",
    "MAE = mean_absolute_error(Y_Train,Y_Train_pred)\n",
    "\n",
    "print(RMSE,MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss(Based on Testing Set): 1145437108.6296737\n",
      "33844.30691874449 21582.245471537353\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "# Stack of Layers\n",
    "model3.add(Dense(units=600, kernel_initializer='normal', activation='relu', input_dim=605))\n",
    "model3.add(Dense(units=500, kernel_initializer='normal', activation='relu'))\n",
    "model3.add(Dense(units=400, kernel_initializer='normal', activation='relu'))\n",
    "model3.add(Dense(units=300, kernel_initializer='normal', activation='relu'))\n",
    "model3.add(Dense(units=1, kernel_initializer='normal'))\n",
    "\n",
    "model3.compile(loss='mean_squared_error',optimizer='adam') \n",
    "\n",
    "Kmod = model3.fit(X_Train_All,Y_Train, epochs=2000, batch_size=20, verbose=0)\n",
    "Error = model3.evaluate(X_Test_All, Y_Test, batch_size=50, verbose=0)\n",
    "print('\\nLoss(Based on Testing Set):', Error)\n",
    "\n",
    "Y_Test_pred = model3.predict(X_Test_All)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Test,Y_Test_pred))\n",
    "MAE = mean_absolute_error(Y_Test,Y_Test_pred)\n",
    "\n",
    "print(RMSE,MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "770.277671549459 502.40806438777827\n"
     ]
    }
   ],
   "source": [
    "Y_Train_pred = model3.predict(X_Train_All)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Train,Y_Train_pred))\n",
    "MAE = mean_absolute_error(Y_Train,Y_Train_pred)\n",
    "\n",
    "print(RMSE,MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We develop three models in total with different number of layers (3, 4 and 5). After 2000 epoches of training, we apply the model to training and testing set respectively and compute the RMSE and MAE.\n",
    "\n",
    "| Model-layers |Test-RMSE   |Test-MAE    |Train-RMSE  |Train-MAE   |\n",
    "|---           |---         |---         |---         |---         | \n",
    "|Three-Layer   |33784.296939|22446.168089|823.322872  |544.217239  |\n",
    "|Four-Layer    |33376.724161|21768.407138|1140.445938 |738.739032  |\n",
    "|Five-Layer    |33844.306918|21582.245471|770.277671  |502.408064  |\n",
    "\n",
    "From the above table, we can see that for models with shallow layers, the gap between training performance and testing performance is smaller. There is a large gap between training and testing performance for model with five layers. The model with deep layers suffers from overfitting problem.\n",
    "\n",
    "Meanwhile there isn't much difference for testing performance with repect to layers changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Evaluate a Narrow Network Topology\n",
    "\n",
    "In this section we evaluate two additional network topologies to check how the number of nodes per layer influnce the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss(Based on Testing Set): 1123073420.4982138\n",
      "33512.28760108963 21589.113500752712\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "\n",
    "# Stack of Layers\n",
    "model4.add(Dense(units=500, kernel_initializer='normal', activation='relu', input_dim=605))\n",
    "model4.add(Dense(units=400, kernel_initializer='normal', activation='relu'))\n",
    "model4.add(Dense(units=300, kernel_initializer='normal', activation='relu'))\n",
    "model4.add(Dense(units=200, kernel_initializer='normal', activation='relu'))\n",
    "model4.add(Dense(units=1, kernel_initializer='normal'))\n",
    "\n",
    "model4.compile(loss='mean_squared_error',optimizer='adam') \n",
    "\n",
    "Kmod = model4.fit(X_Train_All,Y_Train, epochs=2000, batch_size=20, verbose=0)\n",
    "Error = model4.evaluate(X_Test_All, Y_Test, batch_size=50, verbose=0)\n",
    "print('\\nLoss(Based on Testing Set):', Error)\n",
    "\n",
    "Y_Test_pred = model4.predict(X_Test_All)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Test,Y_Test_pred))\n",
    "MAE = mean_absolute_error(Y_Test,Y_Test_pred)\n",
    "\n",
    "print(RMSE,MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "651.1331129392292 396.2108799477605\n"
     ]
    }
   ],
   "source": [
    "Y_Train_pred = model4.predict(X_Train_All)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Train,Y_Train_pred))\n",
    "MAE = mean_absolute_error(Y_Train,Y_Train_pred)\n",
    "\n",
    "print(RMSE,MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss(Based on Testing Set): 1121383894.1462252\n",
      "33487.070664477134 21763.82505629363\n"
     ]
    }
   ],
   "source": [
    "model5 = Sequential()\n",
    "\n",
    "# Stack of Layers\n",
    "model5.add(Dense(units=400, kernel_initializer='normal', activation='relu', input_dim=605))\n",
    "model5.add(Dense(units=300, kernel_initializer='normal', activation='relu'))\n",
    "model5.add(Dense(units=200, kernel_initializer='normal', activation='relu'))\n",
    "model5.add(Dense(units=100, kernel_initializer='normal', activation='relu'))\n",
    "model5.add(Dense(units=1, kernel_initializer='normal'))\n",
    "\n",
    "model5.compile(loss='mean_squared_error',optimizer='adam') \n",
    "\n",
    "Kmod = model5.fit(X_Train_All,Y_Train, epochs=2000, batch_size=20, verbose=0)\n",
    "Error = model5.evaluate(X_Test_All, Y_Test, batch_size=50, verbose=0)\n",
    "print('\\nLoss(Based on Testing Set):', Error)\n",
    "\n",
    "Y_Test_pred = model5.predict(X_Test_All)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Test,Y_Test_pred))\n",
    "MAE = mean_absolute_error(Y_Test,Y_Test_pred)\n",
    "\n",
    "print(RMSE,MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050.0625616956927 745.5063912082858\n"
     ]
    }
   ],
   "source": [
    "Y_Train_pred = model5.predict(X_Train_All)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Train,Y_Train_pred))\n",
    "MAE = mean_absolute_error(Y_Train,Y_Train_pred)\n",
    "\n",
    "print(RMSE,MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same number of layers and continuously shrink the size of layers in the model. The number of nodes in three models are: (600,500,400,300,1) (500,400,300,200,1) (400,300,200,100,1). After 2000 epoches of training, we apply the model to testing set and compute the RMSE and MAE.\n",
    "    \n",
    "| Model-nodes    |Test-RMSE   |Test-MAE    |Train-RMSE  |Train-MAE   |\n",
    "|---             |---         |---         |---         |---         |\n",
    "|Most-nodes      |33844.306918|21582.245471|770.277671  |502.408064  |\n",
    "|Medium-nodes    |33512.287601|21589.113500|651.133112  |396.210879  |\n",
    "|Fewest-nodes    |33487.070664|21763.825056|1050.062561 |745.506391  |\n",
    "\n",
    "From the above table, we can see that there is large gap between training and testing performance in three models. With the number of nodes increases, the performance is getting worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Evaluate a different activation function\n",
    "We change original activation function from \"relu\" to \"tanh\" to check its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss(Based on Testing Set): 1349294330.5434628\n",
      "36732.741703160726 23782.268511691072\n"
     ]
    }
   ],
   "source": [
    "model6 = Sequential()\n",
    "\n",
    "# Stack of Layers\n",
    "model6.add(Dense(units=600, kernel_initializer='normal', activation='tanh', input_dim=605))\n",
    "model6.add(Dense(units=500, kernel_initializer='normal', activation='tanh'))\n",
    "model6.add(Dense(units=400, kernel_initializer='normal', activation='tanh'))\n",
    "model6.add(Dense(units=300, kernel_initializer='normal', activation='tanh'))\n",
    "model6.add(Dense(units=1, kernel_initializer='normal'))\n",
    "\n",
    "model6.compile(loss='mean_squared_error',optimizer='adam') \n",
    "\n",
    "# Training of Model(Train Data is splitted into 80/20)\n",
    "Kmod = model6.fit(X_Train_All,Y_Train, epochs=2000, batch_size=20, verbose=0)\n",
    "Error = model6.evaluate(X_Test_All, Y_Test, batch_size=50, verbose=0)\n",
    "print('\\nLoss(Based on Testing Set):', Error)\n",
    "\n",
    "Y_Test_pred = model6.predict(X_Test_All)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Test,Y_Test_pred))\n",
    "MAE = mean_absolute_error(Y_Test,Y_Test_pred)\n",
    "\n",
    "print(RMSE,MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8977.795195235367 3502.110539259936\n"
     ]
    }
   ],
   "source": [
    "Y_Train_pred = model6.predict(X_Train_All)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Train,Y_Train_pred))\n",
    "MAE = mean_absolute_error(Y_Train,Y_Train_pred)\n",
    "\n",
    "print(RMSE,MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We modify the activation function at each layer from 'relu' to 'tanh' and keep other structure the same. After 2000 epoches of training, we apply the model to testing set and compute the RMSE and MAE.\n",
    "    \n",
    "| Model-acti|Test-RMSE   |Test-MAE    |Train-RMSE  |Train-MAE   |\n",
    "|---        |---         |---         |---         |---         |\n",
    "|tanh       |36732.741703|23782.268511|8977.795195 |3502.110539 |\n",
    "|relu       |33844.306918|21582.245471|770.277671  |502.408064  |\n",
    "\n",
    "From the above table, we can see that the gap between training and testing is smaller with 'tanh' activation function than with 'relu'. Despite this, the 'relu' model also works better. The biggest advantage of relu is the non-saturation of its gradient, which greatly accelerates the convergence of stochastic gradient descent compared to tanh functions. Another nice property is that compared to tanh neurons that involve expensive operations (exponentials, etc.), the relu can be implemented by simply thresholding a matrix of activations at zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Evaluate a different optimization function\n",
    "We change original optimization function from \"adam\" to \"sgd\" to check its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss(Based on Testing Set): 2641556707.9552274\n",
      "51396.07667195328 38918.68632784591\n"
     ]
    }
   ],
   "source": [
    "model7 = Sequential()\n",
    "\n",
    "# Stack of Layers\n",
    "model7.add(Dense(units=600, kernel_initializer='normal', activation='relu', input_dim=605))\n",
    "model7.add(Dense(units=500, kernel_initializer='normal', activation='relu'))\n",
    "model7.add(Dense(units=400, kernel_initializer='normal', activation='relu'))\n",
    "model7.add(Dense(units=300, kernel_initializer='normal', activation='relu'))\n",
    "model7.add(Dense(units=1, kernel_initializer='normal'))\n",
    "\n",
    "model7.compile(loss='mean_squared_error',optimizer='sgd') \n",
    "\n",
    "Kmod = model7.fit(X_Train_All,Y_Train, epochs=2000, batch_size=20, verbose=0)\n",
    "Error = model7.evaluate(X_Test_All, Y_Test, batch_size=50, verbose=0)\n",
    "print('\\nLoss(Based on Testing Set):', Error)\n",
    "\n",
    "Y_Test_pred = model7.predict(X_Test_All)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Test,Y_Test_pred))\n",
    "MAE = mean_absolute_error(Y_Test,Y_Test_pred)\n",
    "\n",
    "print(RMSE,MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51778.060562684725 39101.64227108003\n"
     ]
    }
   ],
   "source": [
    "Y_Train_pred = model7.predict(X_Train_All)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Train,Y_Train_pred))\n",
    "MAE = mean_absolute_error(Y_Train,Y_Train_pred)\n",
    "\n",
    "print(RMSE,MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We modify the optimization function and keep other structures the same. After 2000 epoches of training, we apply the model to testing set and compute the RMSE and MAE.\n",
    "    \n",
    "| Model-opti|Test-RMSE   |Test-MAE    |Train-RMSE  |Train-MAE   |\n",
    "|---        |---         |---         |---         |---         |\n",
    "|sgd        |51396.076671|38918.686327|51778.060562|39101.642271|\n",
    "|adam       |33844.306918|21582.245471|770.277671  |502.408064  |\n",
    "\n",
    "From the above table, we can see that model with 'adam' optimization function works better than 'sgd' function. Adam updates its gradient by taking into account the previous gradient, which is called utilizing the momentum. Momentum can give faster convergence. Momentum simply means that some fraction of the previous update is added to the current update, so that repeated updates in a particular direction compound. Therefore, momentum would help to damp the oscillations in right directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, we can see from the results that a simple neural network can beat the linear models because the variables in the dataset isn't strictly linearly related with YearlyCompensation. However, Neural Network has no real Theory that explains how to choose the number of Hidden Layers except the Thumb Rule and Trial & Error procedure. It takes a lot of time when the Input Data is large, needs powerful computing machines. It is difficult to interpret the results and very hard to interpret and measure the impact of individual predictors. Neural Network also has an Inherent Tendency to overfit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
