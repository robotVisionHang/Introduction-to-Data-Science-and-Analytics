{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import make_scorer, r2_score, mean_squared_error, auc, mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanwen/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (1,3,9,11,13,22,24,25,26,27,28,29,45,57,65,84,86,88,108,110,124,126,151,195,209,224,250,263,265,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,305,307,323,326,327,330,342,372,385,386,394,395) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "Salary = pd.read_csv('./Kaggle_Salary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13995, 605) (9796, 605)\n"
     ]
    }
   ],
   "source": [
    "# Drop three irrelavent columns\n",
    "Salary.drop(['Unnamed: 0','index'], axis=1,inplace=True)\n",
    "\n",
    "QuestionMapping = {}\n",
    "for col in Salary.columns:\n",
    "    QuestionMapping[col] = Salary.loc[0,col]\n",
    "Salary.drop([0],inplace=True)\n",
    "# print(Salary.shape)\n",
    "Salary.head()\n",
    "\n",
    "# See how many null values are in each column\n",
    "# Here we only deal with those colunmns that has null values more than zero but fewer that 1% of all data\n",
    "less_001 = []\n",
    "for col in Salary.columns:\n",
    "    NullNum = Salary[col].isnull().sum()\n",
    "    if (NullNum > 0) & (0.01 * Salary.shape[0] > NullNum):\n",
    "        if (\"Part\" not in col) or (col in ['Q39_Part_1','Q39_Part_2']):\n",
    "            less_001.append(col)\n",
    "# print(\"We drop nan rows in the below columns:\",less_001)\n",
    "\n",
    "for col in less_001:\n",
    "    Salary.dropna(subset=[col],inplace=True)\n",
    "# print(Salary.shape)\n",
    "between_001_020 = []\n",
    "for col in Salary.columns:\n",
    "    NullNum = Salary[col].isnull().sum()\n",
    "    if (0.2 * Salary.shape[0] > NullNum) and (NullNum >= 0.01 * Salary.shape[0]):\n",
    "        if (\"Part\" not in col) or (col in ['Q39_Part_1','Q39_Part_2']):\n",
    "            between_001_020.append(col)\n",
    "# print(\"We use modes in these columns to replace nan value:\",between_001_020)\n",
    "for col in between_001_020:\n",
    "    Salary[col].fillna(Salary[col].mode()[0],inplace=True)\n",
    "\n",
    "between_020_050 = []\n",
    "for col in Salary.columns:\n",
    "    NullNum = Salary[col].isnull().sum()\n",
    "    if (0.2 * Salary.shape[0]) < NullNum and (NullNum < 0.5 * Salary.shape[0]):\n",
    "        if (\"Part\" not in col) or (col in ['Q39_Part_1','Q39_Part_2']):\n",
    "            between_020_050.append(col)\n",
    "# print(\"We set nans in these columns as 'Unknown':\",between_020_050)\n",
    "\n",
    "for col in between_020_050:\n",
    "    Salary.loc[Salary[col].isnull(),col] = 'Unknown'\n",
    "more_050 = []\n",
    "for col in Salary.columns:\n",
    "    NullNum = Salary[col].isnull().sum()\n",
    "    if 0.5 * Salary.shape[0] < NullNum:\n",
    "        if (\"Part\" not in col) or (col in ['Q39_Part_1','Q39_Part_2']):\n",
    "            more_050.append(col)\n",
    "# print(\"We drop these columns because large part of the data is missing:\",more_050)\n",
    "\n",
    "for col in more_050:\n",
    "    Salary.drop([col], axis=1,inplace=True)\n",
    "ALLCOL = Salary.columns.tolist()\n",
    "def select_col(condition, allcol=ALLCOL):\n",
    "    digit = len(condition)\n",
    "    selected = []\n",
    "    for i in allcol:\n",
    "        if condition in i:\n",
    "            if len(i)==digit:\n",
    "                selected.append(i)\n",
    "            elif i[digit] not in '0123456789':\n",
    "                selected.append(i)                \n",
    "    return selected\n",
    "DummyCols = ['Q1','Q4','Q5','Q6','Q7','Q10','Q12_MULTIPLE_CHOICE',\\\n",
    "            'Q17','Q18','Q20','Q23','Q26','Q32','Q39_Part_1', 'Q39_Part_2',\\\n",
    "             'Q40','Q48']\n",
    "for col in DummyCols:\n",
    "    Salary = pd.get_dummies(data=Salary, columns=[col])\n",
    "# print(Salary['Q3'].value_counts())\n",
    "val_counts = Salary['Q3'].value_counts()\n",
    "value_mask = Salary['Q3'].isin(val_counts.index[val_counts < 50])\n",
    "Salary.loc[value_mask,'Q3'] = \"Other\"\n",
    "\n",
    "Salary = pd.get_dummies(data=Salary, columns=['Q3'])\n",
    "# print(Salary['Q22'].value_counts())\n",
    "val_counts = Salary['Q22'].value_counts()\n",
    "value_mask = Salary['Q22'].isin(val_counts.index[val_counts < 100])\n",
    "Salary.loc[value_mask,'Q22'] = \"Other\"\n",
    "\n",
    "Salary = pd.get_dummies(data=Salary, columns=['Q22'])\n",
    "def Range_Normalize(value):\n",
    "    if isinstance(value, str):\n",
    "        if '-' in value:\n",
    "            temp = value.split('-')\n",
    "            return (float(temp[0])+float(temp[1]))/2\n",
    "        if '+' in value:\n",
    "            a = value.split('+')\n",
    "            return float(a[0])\n",
    "        else:\n",
    "            return value\n",
    "    else:\n",
    "        return value\n",
    "# Salary[select_col('Q2')]\n",
    "Salary['Q2_NORMAL'] = Salary['Q2'].apply(Range_Normalize)\n",
    "Salary.drop(['Q2'],axis=1,inplace=True)\n",
    "# Salary[select_col('Q8')]\n",
    "Salary['Q8_NORMAL'] = Salary['Q8'].apply(Range_Normalize)\n",
    "Salary.drop(['Q8'],axis=1,inplace=True)\n",
    "# Salary[select_col('Q24')]\n",
    "def Q24Range_Normalize(value):\n",
    "    mapdict = {'I have never written code and I do not want to learn':-1,\n",
    "               'I have never written code but I want to learn':0,'< 1 year':0.5,\n",
    "              '1-2 years':1.5,'3-5 years':4,'5-10 years':7.5,'10-20 years':15,\n",
    "               '20-30 years':25,'30-40 years':35,'40+ years':40}\n",
    "    return mapdict[value]\n",
    "# Salary[select_col('Q2')]\n",
    "Salary['Q24_NORMAL'] = Salary['Q24'].apply(Q24Range_Normalize)\n",
    "Salary.drop(['Q24'],axis=1,inplace=True)\n",
    "# Salary[select_col('Q25')]\n",
    "Salary['Q25'].unique()\n",
    "def Q25Range_Normalize(value):\n",
    "    mapdict = {'I have never studied machine learning and I do not plan to':-1,\n",
    "               'I have never studied machine learning but plan to learn in the future':0,\n",
    "               '< 1 year':0.5,'1-2 years':1.5,'2-3 years':2.5,'3-4 years':3.5,'4-5 years':4.5,\n",
    "               '5-10 years':7.5,'10-15 years':12.5,'20+ years':20}\n",
    "    return mapdict[value]\n",
    "\n",
    "Salary['Q25_NORMAL'] = Salary['Q25'].apply(Q25Range_Normalize)\n",
    "Salary.drop(['Q25'],axis=1,inplace=True)\n",
    "def Q41Range_Normalize(value):\n",
    "    mapdict = {'Very important':10,\n",
    "               'Slightly important':5,\n",
    "               'No opinion; I do not know':2,\n",
    "               'Unknown':0,\n",
    "               'Not at all important':-10}\n",
    "    return mapdict[value]\n",
    "\n",
    "Salary.loc[Salary['Q41_Part_1'].isnull(),'Q41_Part_1'] = 'Unknown'\n",
    "Salary.loc[Salary['Q41_Part_2'].isnull(),'Q41_Part_2'] = 'Unknown'\n",
    "Salary.loc[Salary['Q41_Part_3'].isnull(),'Q41_Part_3'] = 'Unknown'\n",
    "\n",
    "Salary['Q41_Part_1_NORMAL'] = Salary['Q41_Part_1'].apply(Q41Range_Normalize)\n",
    "Salary.drop(['Q41_Part_1'],axis=1,inplace=True)\n",
    "Salary['Q41_Part_2_NORMAL'] = Salary['Q41_Part_2'].apply(Q41Range_Normalize)\n",
    "Salary.drop(['Q41_Part_2'],axis=1,inplace=True)\n",
    "Salary['Q41_Part_3_NORMAL'] = Salary['Q41_Part_3'].apply(Q41Range_Normalize)\n",
    "Salary.drop(['Q41_Part_3'],axis=1,inplace=True)\n",
    "def Q43Range_Normalize(value):\n",
    "    if isinstance(value, str):\n",
    "        if '-' in value:\n",
    "            temp = value.split('-')\n",
    "            return (float(temp[0])+float(temp[1]))/2\n",
    "        elif value == \"0\":\n",
    "            return 0\n",
    "        else:\n",
    "            return float(\"nan\")\n",
    "    else:\n",
    "        return value\n",
    "Salary['Q43_NORMAL'] = Salary['Q43'].apply(Q43Range_Normalize)\n",
    "Salary.drop(['Q43'],axis=1,inplace=True)\n",
    "Salary['Q43_NORMAL'].fillna(Salary['Q43_NORMAL'].mode()[0],inplace=True)\n",
    "\n",
    "Salary['Q46_NORMAL'] = Salary['Q46'].apply(Q43Range_Normalize)\n",
    "Salary.drop(['Q46'],axis=1,inplace=True)\n",
    "Salary['Q46_NORMAL'].fillna(Salary['Q46_NORMAL'].mode()[0],inplace=True)\n",
    "# Salary[select_col('Q11')]\n",
    "import math\n",
    "def Binarize(value):\n",
    "    if isinstance(value,str):\n",
    "        return 1\n",
    "    else:\n",
    "        if value == 0 or math.isnan(value):\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "def binarize_col(columns, data=Salary):\n",
    "    for col in columns:\n",
    "        data[col] = data[col].apply(Binarize)\n",
    "NormalCols = select_col('Q11_Part') + select_col('Q13_Part') + \\\n",
    "select_col('Q14_Part') + select_col('Q15_Part') + select_col('Q16_Part')\\\n",
    "+ select_col('Q19_Part') + select_col('Q21_Part') + select_col('Q27_Part')\\\n",
    "+ select_col('Q28_Part') + select_col('Q29_Part') + select_col('Q30_Part') +\\\n",
    "select_col('Q31_Part') + select_col('Q33_Part') + select_col('Q34_Part') +\\\n",
    "select_col('Q36_Part') + select_col('Q38_Part') + select_col('Q42_Part') + \\\n",
    "select_col('Q44_Part') + select_col('Q45_Part') + select_col('Q47_Part') + \\\n",
    "select_col('Q49_Part') + select_col('Q50_Part')\n",
    "binarize_col(NormalCols)\n",
    "def TBinarize(value):\n",
    "    if isinstance(value,str):\n",
    "        return float(value)\n",
    "    else:\n",
    "        if math.isnan(value):\n",
    "            return -1\n",
    "        else:\n",
    "            return value\n",
    "def Tbinarize_col(columns, data=Salary):\n",
    "    for col in columns:\n",
    "        data[col] = data[col].apply(TBinarize)\n",
    "\n",
    "Tbinarize_col(select_col('Q34_Part'))\n",
    "Tbinarize_col(select_col('Q35_Part'))\n",
    "for col in Salary.columns:\n",
    "    if isinstance(Salary.loc[1,col],str):\n",
    "        Salary[col] = Salary[col].apply(lambda x:float(x))\n",
    "Salary.rename(columns={\"Q9\": \"Yearly_compensation\"},inplace=True)\n",
    "Salary.rename(columns={\"Time from Start to Finish (seconds)\": \"TimeUse\"},inplace=True)\n",
    "Salary.drop(Salary[Salary.TimeUse < 180].index,inplace=True)\n",
    "for col in Salary.columns:\n",
    "    if len(Salary[col].value_counts())==1:\n",
    "        Salary.drop([col], axis=1,inplace=True)\n",
    "Salary.drop(Salary[Salary.Yearly_compensation < 1000].index, inplace=True)\n",
    "Salary.drop(Salary[Salary.Yearly_compensation > 350000].index, inplace=True)\n",
    "X = Salary.drop(['Yearly_compensation'],axis=1)\n",
    "Y = Salary['Yearly_compensation']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X,Y,test_size=0.3, random_state=20)\n",
    "print(X.shape,X_Train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9796, 605)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "def feature_scaling(X_Train, X_Test, method):\n",
    "    methodmap = {'z-score':StandardScaler(),'min-max':MinMaxScaler(),'robust':RobustScaler()}\n",
    "    if method not in ['z-score','min-max','robust']:\n",
    "        print(\"We don't have this kind of method for normalization. Please choose from ['z-score','min-max','robust']\")\n",
    "        return pd.DataFrame(X_Train), pd.DataFrame(X_Test)\n",
    "    else:\n",
    "        ss = methodmap[method].fit(X_Train)\n",
    "        X_Train = ss.transform(X_Train)\n",
    "        X_Test = ss.transform(X_Test)\n",
    "        return pd.DataFrame(X_Train), pd.DataFrame(X_Test)\n",
    "X_Train_All, X_Test_All = feature_scaling(X_Train,X_Test,'z-score')\n",
    "print(X_Train_All.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanwen/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Learner(Vanilla/Basic Model)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# import sklearn.neural_network as sknn\n",
    "# import sklearn.model_selection as modsel\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=605, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=1000, batch_size=20, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model with standardized dataset\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=50, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss(Based on Testing Set): 1136172527.7065969\n",
      "-22470222.65762509 243531035.18178013\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "\n",
    "# Stack of Layers\n",
    "model1.add(Dense(units=600, kernel_initializer='normal', activation='relu', input_dim=605))\n",
    "model1.add(Dense(units=500, kernel_initializer='normal', activation='relu'))\n",
    "model1.add(Dense(units=1, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "model1.compile(loss='mean_squared_error',optimizer='adam') \n",
    "\n",
    "Kmod = model1.fit(X_Train_All,Y_Train, epochs=1000, batch_size=20, verbose=0)\n",
    "Error = model1.evaluate(X_Test_All, Y_Test, batch_size=50, verbose=0)\n",
    "print('\\nLoss(Based on Testing Set):', Error)\n",
    "\n",
    "Y_Test_pred = model1.predict(X_Test)\n",
    "R2 = r2_score(Y_Test,Y_Test_pred)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Test,Y_Test_pred))\n",
    "\n",
    "print(R2,RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss(Based on Testing Set): 1156815113.830912\n",
      "-5073535.138061203 115719352.8687122\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "# Stack of Layers\n",
    "model2.add(Dense(units=600, kernel_initializer='normal', activation='relu', input_dim=605))\n",
    "model2.add(Dense(units=500, kernel_initializer='normal', activation='relu'))\n",
    "model2.add(Dense(units=400, kernel_initializer='normal', activation='relu'))\n",
    "model2.add(Dense(units=1, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "model2.compile(loss='mean_squared_error',optimizer='adam') \n",
    "\n",
    "# Training of Model(Train Data is splitted into 80/20)\n",
    "Kmod = model2.fit(X_Train_All,Y_Train, epochs=1000, batch_size=20, verbose=0)\n",
    "Error = model2.evaluate(X_Test_All, Y_Test, batch_size=50, verbose=0)\n",
    "print('\\nLoss(Based on Testing Set):', Error)\n",
    "\n",
    "Y_Test_pred = model2.predict(X_Test)\n",
    "R2 = r2_score(Y_Test,Y_Test_pred)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Test,Y_Test_pred))\n",
    "\n",
    "print(R2,RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss(Based on Testing Set): 1110568818.0080972\n",
      "-4615403.509880863 110371133.38378283\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "# Stack of Layers\n",
    "model3.add(Dense(units=600, kernel_initializer='normal', activation='relu', input_dim=605))\n",
    "model3.add(Dense(units=500, kernel_initializer='normal', activation='relu'))\n",
    "model3.add(Dense(units=400, kernel_initializer='normal', activation='relu'))\n",
    "model3.add(Dense(units=300, kernel_initializer='normal', activation='relu'))\n",
    "model3.add(Dense(units=1, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "model3.compile(loss='mean_squared_error',optimizer='adam') \n",
    "\n",
    "# Training of Model(Train Data is splitted into 80/20)\n",
    "Kmod = model3.fit(X_Train_All,Y_Train, epochs=1000, batch_size=20, verbose=0)\n",
    "Error = model3.evaluate(X_Test_All, Y_Test, batch_size=50, verbose=0)\n",
    "print('\\nLoss(Based on Testing Set):', Error)\n",
    "\n",
    "Y_Test_pred = model3.predict(X_Test)\n",
    "R2 = r2_score(Y_Test,Y_Test_pred)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Test,Y_Test_pred))\n",
    "\n",
    "print(R2,RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss(Based on Testing Set): 1108455339.7894738\n",
      "-5853622.170090846 124297672.87373401\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "\n",
    "# Stack of Layers\n",
    "model4.add(Dense(units=500, kernel_initializer='normal', activation='relu', input_dim=605))\n",
    "model4.add(Dense(units=400, kernel_initializer='normal', activation='relu'))\n",
    "model4.add(Dense(units=300, kernel_initializer='normal', activation='relu'))\n",
    "model4.add(Dense(units=200, kernel_initializer='normal', activation='relu'))\n",
    "model4.add(Dense(units=1, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "model4.compile(loss='mean_squared_error',optimizer='adam') \n",
    "\n",
    "# Training of Model(Train Data is splitted into 80/20)\n",
    "Kmod = model4.fit(X_Train_All,Y_Train, epochs=1000, batch_size=20, verbose=0)\n",
    "Error = model4.evaluate(X_Test_All, Y_Test, batch_size=50, verbose=0)\n",
    "print('\\nLoss(Based on Testing Set):', Error)\n",
    "\n",
    "Y_Test_pred = model4.predict(X_Test)\n",
    "R2 = r2_score(Y_Test,Y_Test_pred)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Test,Y_Test_pred))\n",
    "\n",
    "print(R2,RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss(Based on Testing Set): 1136231052.7268398\n",
      "-4770835.258522127 112214214.26971562\n"
     ]
    }
   ],
   "source": [
    "model5 = Sequential()\n",
    "\n",
    "# Stack of Layers\n",
    "model5.add(Dense(units=400, kernel_initializer='normal', activation='relu', input_dim=605))\n",
    "model5.add(Dense(units=300, kernel_initializer='normal', activation='relu'))\n",
    "model5.add(Dense(units=200, kernel_initializer='normal', activation='relu'))\n",
    "model5.add(Dense(units=100, kernel_initializer='normal', activation='relu'))\n",
    "model5.add(Dense(units=1, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "model5.compile(loss='mean_squared_error',optimizer='adam') \n",
    "\n",
    "# Training of Model(Train Data is splitted into 80/20)\n",
    "Kmod = model5.fit(X_Train_All,Y_Train, epochs=1000, batch_size=20, verbose=0)\n",
    "Error = model5.evaluate(X_Test_All, Y_Test, batch_size=50, verbose=0)\n",
    "print('\\nLoss(Based on Testing Set):', Error)\n",
    "\n",
    "Y_Test_pred = model5.predict(X_Test)\n",
    "R2 = r2_score(Y_Test,Y_Test_pred)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Test,Y_Test_pred))\n",
    "\n",
    "print(R2,RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss(Based on Testing Set): 1363606424.8897357\n",
      "-0.46488337061081286 62180.20387825402\n"
     ]
    }
   ],
   "source": [
    "model6 = Sequential()\n",
    "\n",
    "# Stack of Layers\n",
    "model6.add(Dense(units=600, kernel_initializer='normal', activation='tanh', input_dim=605))\n",
    "model6.add(Dense(units=500, kernel_initializer='normal', activation='tanh'))\n",
    "model6.add(Dense(units=400, kernel_initializer='normal', activation='tanh'))\n",
    "model6.add(Dense(units=300, kernel_initializer='normal', activation='tanh'))\n",
    "model6.add(Dense(units=1, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "model6.compile(loss='mean_squared_error',optimizer='adam') \n",
    "\n",
    "# Training of Model(Train Data is splitted into 80/20)\n",
    "Kmod = model6.fit(X_Train_All,Y_Train, epochs=1000, batch_size=20, verbose=0)\n",
    "Error = model6.evaluate(X_Test_All, Y_Test, batch_size=50, verbose=0)\n",
    "print('\\nLoss(Based on Testing Set):', Error)\n",
    "\n",
    "Y_Test_pred = model6.predict(X_Test)\n",
    "R2 = r2_score(Y_Test,Y_Test_pred)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Test,Y_Test_pred))\n",
    "\n",
    "print(R2,RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss(Based on Testing Set): 1468834656.9068825\n",
      "-1.6002589262059388 82843.59086671998\n"
     ]
    }
   ],
   "source": [
    "# Training of Model(Train Data is splitted into 80/20)\n",
    "Kmod = model6.fit(X_Train_All,Y_Train, epochs=3000, batch_size=20, verbose=0)\n",
    "Error = model6.evaluate(X_Test_All, Y_Test, batch_size=50, verbose=0)\n",
    "print('\\nLoss(Based on Testing Set):', Error)\n",
    "\n",
    "Y_Test_pred = model6.predict(X_Test)\n",
    "R2 = r2_score(Y_Test,Y_Test_pred)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Test,Y_Test_pred))\n",
    "\n",
    "print(R2,RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss(Based on Testing Set): 5155631543.266492\n",
      "-0.9533525525069886 71802.72630012724\n"
     ]
    }
   ],
   "source": [
    "model7 = Sequential()\n",
    "\n",
    "# Stack of Layers\n",
    "model7.add(Dense(units=600, kernel_initializer='normal', activation='relu', input_dim=605))\n",
    "model7.add(Dense(units=500, kernel_initializer='normal', activation='relu'))\n",
    "model7.add(Dense(units=400, kernel_initializer='normal', activation='relu'))\n",
    "model7.add(Dense(units=300, kernel_initializer='normal', activation='relu'))\n",
    "model7.add(Dense(units=1, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "model7.compile(loss='mean_squared_error',optimizer='sgd') \n",
    "\n",
    "# Training of Model(Train Data is splitted into 80/20)\n",
    "Kmod = model7.fit(X_Train_All,Y_Train, epochs=1000, batch_size=20, verbose=0)\n",
    "Error = model7.evaluate(X_Test_All, Y_Test, batch_size=50, verbose=0)\n",
    "print('\\nLoss(Based on Testing Set):', Error)\n",
    "\n",
    "Y_Test_pred = model7.predict(X_Test)\n",
    "R2 = r2_score(Y_Test,Y_Test_pred)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Test,Y_Test_pred))\n",
    "\n",
    "print(R2,RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss(Based on Testing Set): 5155631543.266492\n",
      "-0.9533525525069886 71802.72630012724\n"
     ]
    }
   ],
   "source": [
    "model8 = Sequential()\n",
    "\n",
    "# Stack of Layers\n",
    "model8.add(Dense(units=600, kernel_initializer='normal', activation='tanh', input_dim=605))\n",
    "model8.add(Dense(units=500, kernel_initializer='normal', activation='tanh'))\n",
    "model8.add(Dense(units=400, kernel_initializer='normal', activation='tanh'))\n",
    "model8.add(Dense(units=300, kernel_initializer='normal', activation='tanh'))\n",
    "model8.add(Dense(units=1, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "model8.compile(loss='mean_squared_error',optimizer='sgd') \n",
    "\n",
    "# Training of Model(Train Data is splitted into 80/20)\n",
    "Kmod = model8.fit(X_Train_All,Y_Train, epochs=1000, batch_size=5, verbose=0)\n",
    "Error = model8.evaluate(X_Test_All, Y_Test, batch_size=50, verbose=0)\n",
    "print('\\nLoss(Based on Testing Set):', Error)\n",
    "\n",
    "Y_Test_pred = model8.predict(X_Test)\n",
    "R2 = r2_score(Y_Test,Y_Test_pred)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Test,Y_Test_pred))\n",
    "\n",
    "print(R2,RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss(Based on Testing Set): 5155631543.266492\n",
      "-0.9533525525069886 71802.72630012724\n"
     ]
    }
   ],
   "source": [
    "# Training of Model(Train Data is splitted into 80/20)\n",
    "Kmod = model8.fit(X_Train_All,Y_Train, epochs=1000, batch_size=20, verbose=0)\n",
    "Error = model8.evaluate(X_Test_All, Y_Test, batch_size=50, verbose=0)\n",
    "print('\\nLoss(Based on Testing Set):', Error)\n",
    "\n",
    "Y_Test_pred = model8.predict(X_Test)\n",
    "R2 = r2_score(Y_Test,Y_Test_pred)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Test,Y_Test_pred))\n",
    "\n",
    "print(R2,RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8 = Sequential()\n",
    "\n",
    "# Stack of Layers\n",
    "model8.add(Dense(units=600, kernel_initializer='normal', activation='tanh', input_dim=605))\n",
    "model8.add(Dense(units=500, kernel_initializer='normal', activation='tanh'))\n",
    "model8.add(Dense(units=400, kernel_initializer='normal', activation='tanh'))\n",
    "model8.add(Dense(units=300, kernel_initializer='normal', activation='tanh'))\n",
    "model8.add(Dense(units=1, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "model8.compile(loss='mean_squared_error',optimizer='sgd') \n",
    "\n",
    "# Training of Model(Train Data is splitted into 80/20)\n",
    "Kmod = model8.fit(X_Train_All,Y_Train, epochs=1000, batch_size=5, verbose=0)\n",
    "Error = model8.evaluate(X_Test_All, Y_Test, batch_size=50, verbose=0)\n",
    "print('\\nLoss(Based on Testing Set):', Error)\n",
    "\n",
    "Y_Test_pred = model8.predict(X_Test)\n",
    "R2 = r2_score(Y_Test,Y_Test_pred)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Test,Y_Test_pred))\n",
    "\n",
    "print(R2,RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown activation function:identity",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-b36700d7cd61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Stack of Layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'identity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m605\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'identity'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'identity'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/activations.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0midentifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/activations.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(name, custom_objects)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         printable_module_name='activation function')\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 raise ValueError('Unknown ' + printable_module_name +\n\u001b[0;32m--> 165\u001b[0;31m                                  ':' + function_name)\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown activation function:identity"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Stack of Layers\n",
    "model.add(Dense(units=600, activation='tanh', input_dim=605))\n",
    "model.add(Dense(units=500, activation='tanh'))\n",
    "model.add(Dense(units=400, activation='tanh'))\n",
    "model.add(Dense(units=300, activation='tanh'))\n",
    "\n",
    "model.add(Dense(units=1, activation='relu'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "               optimizer='sgd') \n",
    "\n",
    "# Training of Model(Train Data is splitted into 80/20)\n",
    "Kmod = model.fit(X_Train_All,Y_Train, epochs=1000, batch_size=20, verbose=0)\n",
    "Error = model.evaluate(X_Test_All, Y_Test, batch_size=50, verbose=0)\n",
    "print('\\nLoss(Based on Testing Set):', Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss(Based on Testing Set): 3843737328.758276\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Stack of Layers\n",
    "model.add(Dense(units=600, activation='tanh', input_dim=605))\n",
    "model.add(Dense(units=500, activation='tanh'))\n",
    "model.add(Dense(units=400, activation='tanh'))\n",
    "model.add(Dense(units=300, activation='tanh'))\n",
    "model.add(Dense(units=200, activation='tanh'))\n",
    "\n",
    "model.add(Dense(units=1, activation='relu'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "               optimizer='adam') \n",
    "\n",
    "# Training of Model(Train Data is splitted into 80/20)\n",
    "Kmod = model.fit(X_Train_All,Y_Train, epochs=1000, batch_size=20, verbose=0)\n",
    "Error = model.evaluate(X_Test_All, Y_Test, batch_size=50, verbose=0)\n",
    "print('\\nLoss(Based on Testing Set):', Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss(Based on Testing Set): 3979814794.6387234\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Stack of Layers\n",
    "model.add(Dense(units=600, activation='tanh', input_dim=605))\n",
    "model.add(Dense(units=500, activation='tanh'))\n",
    "model.add(Dense(units=400, activation='tanh'))\n",
    "model.add(Dense(units=300, activation='tanh'))\n",
    "model.add(Dense(units=200, activation='tanh'))\n",
    "model.add(Dense(units=200, activation='tanh'))\n",
    "model.add(Dense(units=200, activation='tanh'))\n",
    "\n",
    "model.add(Dense(units=1, activation='relu'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "               optimizer='adam') \n",
    "\n",
    "# Training of Model(Train Data is splitted into 80/20)\n",
    "Kmod = model.fit(X_Train_All,Y_Train, epochs=2000, batch_size=20, verbose=0)\n",
    "Error = model.evaluate(X_Test_All, Y_Test, batch_size=50, verbose=0)\n",
    "print('\\nLoss(Based on Testing Set):', Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanwen/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(600, 300, 150, 75), learning_rate='constant',\n",
       "       learning_rate_init=0.0002, max_iter=10000, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=20, shuffle=True,\n",
       "       solver='sgd', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "clf = MLPRegressor(hidden_layer_sizes=(600,300,150,75), activation='relu', solver='sgd', alpha=0.0001, \\\n",
    "                   learning_rate_init=0.0002,max_iter=10000, shuffle=True, random_state=20, early_stopping=False, \\\n",
    "                   beta_1=0.9, beta_2=0.999)\n",
    "# (solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(10,8,4), random_state=1, max_iter=1000)\n",
    "clf.fit(X_Train_All, Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ea7f8bde8c5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mY_Test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_Test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mR2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_Test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_Test_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mRMSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_Test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_Test_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36mr2_score\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \"\"\"\n\u001b[1;32m    529\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 530\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "Y_Test_pred = clf.predict(X_Test)\n",
    "R2 = r2_score(Y_Test,Y_Test_pred)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Test,Y_Test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
