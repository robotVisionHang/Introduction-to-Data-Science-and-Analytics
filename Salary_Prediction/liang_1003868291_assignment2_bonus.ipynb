{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import make_scorer, r2_score, mean_squared_error, auc, mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanwen/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (1,3,9,11,13,22,24,25,26,27,28,29,45,57,65,84,86,88,108,110,124,126,151,195,209,224,250,263,265,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,305,307,323,326,327,330,342,372,385,386,394,395) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "Salary = pd.read_csv('./Kaggle_Salary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13995, 605) (9796, 605)\n"
     ]
    }
   ],
   "source": [
    "# Drop three irrelavent columns\n",
    "Salary.drop(['Unnamed: 0','index'], axis=1,inplace=True)\n",
    "\n",
    "QuestionMapping = {}\n",
    "for col in Salary.columns:\n",
    "    QuestionMapping[col] = Salary.loc[0,col]\n",
    "Salary.drop([0],inplace=True)\n",
    "# print(Salary.shape)\n",
    "Salary.head()\n",
    "\n",
    "# See how many null values are in each column\n",
    "# Here we only deal with those colunmns that has null values more than zero but fewer that 1% of all data\n",
    "less_001 = []\n",
    "for col in Salary.columns:\n",
    "    NullNum = Salary[col].isnull().sum()\n",
    "    if (NullNum > 0) & (0.01 * Salary.shape[0] > NullNum):\n",
    "        if (\"Part\" not in col) or (col in ['Q39_Part_1','Q39_Part_2']):\n",
    "            less_001.append(col)\n",
    "# print(\"We drop nan rows in the below columns:\",less_001)\n",
    "\n",
    "for col in less_001:\n",
    "    Salary.dropna(subset=[col],inplace=True)\n",
    "# print(Salary.shape)\n",
    "between_001_020 = []\n",
    "for col in Salary.columns:\n",
    "    NullNum = Salary[col].isnull().sum()\n",
    "    if (0.2 * Salary.shape[0] > NullNum) and (NullNum >= 0.01 * Salary.shape[0]):\n",
    "        if (\"Part\" not in col) or (col in ['Q39_Part_1','Q39_Part_2']):\n",
    "            between_001_020.append(col)\n",
    "# print(\"We use modes in these columns to replace nan value:\",between_001_020)\n",
    "for col in between_001_020:\n",
    "    Salary[col].fillna(Salary[col].mode()[0],inplace=True)\n",
    "\n",
    "between_020_050 = []\n",
    "for col in Salary.columns:\n",
    "    NullNum = Salary[col].isnull().sum()\n",
    "    if (0.2 * Salary.shape[0]) < NullNum and (NullNum < 0.5 * Salary.shape[0]):\n",
    "        if (\"Part\" not in col) or (col in ['Q39_Part_1','Q39_Part_2']):\n",
    "            between_020_050.append(col)\n",
    "# print(\"We set nans in these columns as 'Unknown':\",between_020_050)\n",
    "\n",
    "for col in between_020_050:\n",
    "    Salary.loc[Salary[col].isnull(),col] = 'Unknown'\n",
    "more_050 = []\n",
    "for col in Salary.columns:\n",
    "    NullNum = Salary[col].isnull().sum()\n",
    "    if 0.5 * Salary.shape[0] < NullNum:\n",
    "        if (\"Part\" not in col) or (col in ['Q39_Part_1','Q39_Part_2']):\n",
    "            more_050.append(col)\n",
    "# print(\"We drop these columns because large part of the data is missing:\",more_050)\n",
    "\n",
    "for col in more_050:\n",
    "    Salary.drop([col], axis=1,inplace=True)\n",
    "ALLCOL = Salary.columns.tolist()\n",
    "def select_col(condition, allcol=ALLCOL):\n",
    "    digit = len(condition)\n",
    "    selected = []\n",
    "    for i in allcol:\n",
    "        if condition in i:\n",
    "            if len(i)==digit:\n",
    "                selected.append(i)\n",
    "            elif i[digit] not in '0123456789':\n",
    "                selected.append(i)                \n",
    "    return selected\n",
    "DummyCols = ['Q1','Q4','Q5','Q6','Q7','Q10','Q12_MULTIPLE_CHOICE',\\\n",
    "            'Q17','Q18','Q20','Q23','Q26','Q32','Q39_Part_1', 'Q39_Part_2',\\\n",
    "             'Q40','Q48']\n",
    "for col in DummyCols:\n",
    "    Salary = pd.get_dummies(data=Salary, columns=[col])\n",
    "# print(Salary['Q3'].value_counts())\n",
    "val_counts = Salary['Q3'].value_counts()\n",
    "value_mask = Salary['Q3'].isin(val_counts.index[val_counts < 50])\n",
    "Salary.loc[value_mask,'Q3'] = \"Other\"\n",
    "\n",
    "Salary = pd.get_dummies(data=Salary, columns=['Q3'])\n",
    "# print(Salary['Q22'].value_counts())\n",
    "val_counts = Salary['Q22'].value_counts()\n",
    "value_mask = Salary['Q22'].isin(val_counts.index[val_counts < 100])\n",
    "Salary.loc[value_mask,'Q22'] = \"Other\"\n",
    "\n",
    "Salary = pd.get_dummies(data=Salary, columns=['Q22'])\n",
    "def Range_Normalize(value):\n",
    "    if isinstance(value, str):\n",
    "        if '-' in value:\n",
    "            temp = value.split('-')\n",
    "            return (float(temp[0])+float(temp[1]))/2\n",
    "        if '+' in value:\n",
    "            a = value.split('+')\n",
    "            return float(a[0])\n",
    "        else:\n",
    "            return value\n",
    "    else:\n",
    "        return value\n",
    "# Salary[select_col('Q2')]\n",
    "Salary['Q2_NORMAL'] = Salary['Q2'].apply(Range_Normalize)\n",
    "Salary.drop(['Q2'],axis=1,inplace=True)\n",
    "# Salary[select_col('Q8')]\n",
    "Salary['Q8_NORMAL'] = Salary['Q8'].apply(Range_Normalize)\n",
    "Salary.drop(['Q8'],axis=1,inplace=True)\n",
    "# Salary[select_col('Q24')]\n",
    "def Q24Range_Normalize(value):\n",
    "    mapdict = {'I have never written code and I do not want to learn':-1,\n",
    "               'I have never written code but I want to learn':0,'< 1 year':0.5,\n",
    "              '1-2 years':1.5,'3-5 years':4,'5-10 years':7.5,'10-20 years':15,\n",
    "               '20-30 years':25,'30-40 years':35,'40+ years':40}\n",
    "    return mapdict[value]\n",
    "# Salary[select_col('Q2')]\n",
    "Salary['Q24_NORMAL'] = Salary['Q24'].apply(Q24Range_Normalize)\n",
    "Salary.drop(['Q24'],axis=1,inplace=True)\n",
    "# Salary[select_col('Q25')]\n",
    "Salary['Q25'].unique()\n",
    "def Q25Range_Normalize(value):\n",
    "    mapdict = {'I have never studied machine learning and I do not plan to':-1,\n",
    "               'I have never studied machine learning but plan to learn in the future':0,\n",
    "               '< 1 year':0.5,'1-2 years':1.5,'2-3 years':2.5,'3-4 years':3.5,'4-5 years':4.5,\n",
    "               '5-10 years':7.5,'10-15 years':12.5,'20+ years':20}\n",
    "    return mapdict[value]\n",
    "\n",
    "Salary['Q25_NORMAL'] = Salary['Q25'].apply(Q25Range_Normalize)\n",
    "Salary.drop(['Q25'],axis=1,inplace=True)\n",
    "def Q41Range_Normalize(value):\n",
    "    mapdict = {'Very important':10,\n",
    "               'Slightly important':5,\n",
    "               'No opinion; I do not know':2,\n",
    "               'Unknown':0,\n",
    "               'Not at all important':-10}\n",
    "    return mapdict[value]\n",
    "\n",
    "Salary.loc[Salary['Q41_Part_1'].isnull(),'Q41_Part_1'] = 'Unknown'\n",
    "Salary.loc[Salary['Q41_Part_2'].isnull(),'Q41_Part_2'] = 'Unknown'\n",
    "Salary.loc[Salary['Q41_Part_3'].isnull(),'Q41_Part_3'] = 'Unknown'\n",
    "\n",
    "Salary['Q41_Part_1_NORMAL'] = Salary['Q41_Part_1'].apply(Q41Range_Normalize)\n",
    "Salary.drop(['Q41_Part_1'],axis=1,inplace=True)\n",
    "Salary['Q41_Part_2_NORMAL'] = Salary['Q41_Part_2'].apply(Q41Range_Normalize)\n",
    "Salary.drop(['Q41_Part_2'],axis=1,inplace=True)\n",
    "Salary['Q41_Part_3_NORMAL'] = Salary['Q41_Part_3'].apply(Q41Range_Normalize)\n",
    "Salary.drop(['Q41_Part_3'],axis=1,inplace=True)\n",
    "def Q43Range_Normalize(value):\n",
    "    if isinstance(value, str):\n",
    "        if '-' in value:\n",
    "            temp = value.split('-')\n",
    "            return (float(temp[0])+float(temp[1]))/2\n",
    "        elif value == \"0\":\n",
    "            return 0\n",
    "        else:\n",
    "            return float(\"nan\")\n",
    "    else:\n",
    "        return value\n",
    "Salary['Q43_NORMAL'] = Salary['Q43'].apply(Q43Range_Normalize)\n",
    "Salary.drop(['Q43'],axis=1,inplace=True)\n",
    "Salary['Q43_NORMAL'].fillna(Salary['Q43_NORMAL'].mode()[0],inplace=True)\n",
    "\n",
    "Salary['Q46_NORMAL'] = Salary['Q46'].apply(Q43Range_Normalize)\n",
    "Salary.drop(['Q46'],axis=1,inplace=True)\n",
    "Salary['Q46_NORMAL'].fillna(Salary['Q46_NORMAL'].mode()[0],inplace=True)\n",
    "# Salary[select_col('Q11')]\n",
    "import math\n",
    "def Binarize(value):\n",
    "    if isinstance(value,str):\n",
    "        return 1\n",
    "    else:\n",
    "        if value == 0 or math.isnan(value):\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "def binarize_col(columns, data=Salary):\n",
    "    for col in columns:\n",
    "        data[col] = data[col].apply(Binarize)\n",
    "NormalCols = select_col('Q11_Part') + select_col('Q13_Part') + \\\n",
    "select_col('Q14_Part') + select_col('Q15_Part') + select_col('Q16_Part')\\\n",
    "+ select_col('Q19_Part') + select_col('Q21_Part') + select_col('Q27_Part')\\\n",
    "+ select_col('Q28_Part') + select_col('Q29_Part') + select_col('Q30_Part') +\\\n",
    "select_col('Q31_Part') + select_col('Q33_Part') + select_col('Q34_Part') +\\\n",
    "select_col('Q36_Part') + select_col('Q38_Part') + select_col('Q42_Part') + \\\n",
    "select_col('Q44_Part') + select_col('Q45_Part') + select_col('Q47_Part') + \\\n",
    "select_col('Q49_Part') + select_col('Q50_Part')\n",
    "binarize_col(NormalCols)\n",
    "def TBinarize(value):\n",
    "    if isinstance(value,str):\n",
    "        return float(value)\n",
    "    else:\n",
    "        if math.isnan(value):\n",
    "            return -1\n",
    "        else:\n",
    "            return value\n",
    "def Tbinarize_col(columns, data=Salary):\n",
    "    for col in columns:\n",
    "        data[col] = data[col].apply(TBinarize)\n",
    "\n",
    "Tbinarize_col(select_col('Q34_Part'))\n",
    "Tbinarize_col(select_col('Q35_Part'))\n",
    "for col in Salary.columns:\n",
    "    if isinstance(Salary.loc[1,col],str):\n",
    "        Salary[col] = Salary[col].apply(lambda x:float(x))\n",
    "Salary.rename(columns={\"Q9\": \"Yearly_compensation\"},inplace=True)\n",
    "Salary.rename(columns={\"Time from Start to Finish (seconds)\": \"TimeUse\"},inplace=True)\n",
    "Salary.drop(Salary[Salary.TimeUse < 180].index,inplace=True)\n",
    "for col in Salary.columns:\n",
    "    if len(Salary[col].value_counts())==1:\n",
    "        Salary.drop([col], axis=1,inplace=True)\n",
    "Salary.drop(Salary[Salary.Yearly_compensation < 1000].index, inplace=True)\n",
    "Salary.drop(Salary[Salary.Yearly_compensation > 350000].index, inplace=True)\n",
    "X = Salary.drop(['Yearly_compensation'],axis=1)\n",
    "Y = Salary['Yearly_compensation']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X,Y,test_size=0.3, random_state=20)\n",
    "print(X.shape,X_Train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "def feature_scaling(X_Train, X_Test, method):\n",
    "    methodmap = {'z-score':StandardScaler(),'min-max':MinMaxScaler(),'robust':RobustScaler()}\n",
    "    if method not in ['z-score','min-max','robust']:\n",
    "        print(\"We don't have this kind of method for normalization. Please choose from ['z-score','min-max','robust']\")\n",
    "        return pd.DataFrame(X_Train), pd.DataFrame(X_Test)\n",
    "    else:\n",
    "        ss = methodmap[method].fit(X_Train)\n",
    "        X_Train = ss.transform(X_Train)\n",
    "        X_Test = ss.transform(X_Test)\n",
    "        return pd.DataFrame(X_Train), pd.DataFrame(X_Test)\n",
    "X_Train_All, X_Test_All = feature_scaling(X_Train,X_Test,'z-score')\n",
    "print(X_Train_All.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Learner(Vanilla/Basic Model)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import sklearn.neural_network as sknn\n",
    "import sklearn.model_selection as modsel\n",
    "\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Stack of Layers\n",
    "model.add(Dense(units=600, activation='relu', input_dim=605))\n",
    "model.add(Dense(units=500, activation='relu'))\n",
    "model.add(Dense(units=1, activation='relu'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "               optimizer='adam') \n",
    "\n",
    "# Training of Model(Train Data is splitted into 80/20)\n",
    "Kmod = model.fit(X_Train_All,Y_Train, epochs=1000, batch_size=5, verbose=0)\n",
    "Error = model.evaluate(X_Test_All, Y_Test, batch_size=50, verbose=0)\n",
    "print('\\nLoss(Based on Testing Set):', Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_Test_pred = model.predict(X_Test)\n",
    "R2 = r2_score(Y_Test,Y_Test_pred)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Test,Y_Test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-19336910.21726435 225914532.49072984\n"
     ]
    }
   ],
   "source": [
    "print(R2,RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5586224. ],\n",
       "       [11137876. ],\n",
       "       [ 5258918.5],\n",
       "       ...,\n",
       "       [ 4582865. ],\n",
       "       [ 2008258.4],\n",
       "       [13097540. ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_Test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanwen/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(600, 300, 150, 75), learning_rate='constant',\n",
       "       learning_rate_init=0.0002, max_iter=10000, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=20, shuffle=True,\n",
       "       solver='sgd', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "clf = MLPRegressor(hidden_layer_sizes=(600,300,150,75), activation='relu', solver='sgd', alpha=0.0001, \\\n",
    "                   learning_rate_init=0.0002,max_iter=10000, shuffle=True, random_state=20, early_stopping=False, \\\n",
    "                   beta_1=0.9, beta_2=0.999)\n",
    "# (solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(10,8,4), random_state=1, max_iter=1000)\n",
    "clf.fit(X_Train_All, Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ea7f8bde8c5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mY_Test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_Test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mR2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_Test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_Test_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mRMSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_Test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_Test_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36mr2_score\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \"\"\"\n\u001b[1;32m    529\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 530\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "Y_Test_pred = clf.predict(X_Test)\n",
    "R2 = r2_score(Y_Test,Y_Test_pred)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Test,Y_Test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
