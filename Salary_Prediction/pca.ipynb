{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import make_scorer, r2_score, mean_squared_error, auc, mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanwen/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (1,3,9,11,13,22,24,25,26,27,28,29,45,57,65,84,86,88,108,110,124,126,151,195,209,224,250,263,265,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,305,307,323,326,327,330,342,372,385,386,394,395) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "Salary = pd.read_csv('./Kaggle_Salary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15429, 395)\n",
      "We drop nan rows in the below columns: ['Q5', 'Q8']\n",
      "(15219, 395)\n",
      "We use modes in these columns to replace nan value: ['Q10', 'Q12_MULTIPLE_CHOICE', 'Q18', 'Q23', 'Q24', 'Q25', 'Q26']\n",
      "We set nans in these columns as 'Unknown': ['Q17', 'Q20', 'Q22', 'Q32', 'Q39_Part_1', 'Q39_Part_2', 'Q40', 'Q43', 'Q46', 'Q48']\n",
      "We drop these columns because large part of the data is missing: ['Q37']\n",
      "(13995, 605) (9796, 605)\n"
     ]
    }
   ],
   "source": [
    "# Drop three irrelavent columns\n",
    "Salary.drop(['Unnamed: 0','index'], axis=1,inplace=True)\n",
    "\n",
    "QuestionMapping = {}\n",
    "for col in Salary.columns:\n",
    "    QuestionMapping[col] = Salary.loc[0,col]\n",
    "Salary.drop([0],inplace=True)\n",
    "print(Salary.shape)\n",
    "Salary.head()\n",
    "\n",
    "# See how many null values are in each column\n",
    "# Here we only deal with those colunmns that has null values more than zero but fewer that 1% of all data\n",
    "less_001 = []\n",
    "for col in Salary.columns:\n",
    "    NullNum = Salary[col].isnull().sum()\n",
    "    if (NullNum > 0) & (0.01 * Salary.shape[0] > NullNum):\n",
    "        if (\"Part\" not in col) or (col in ['Q39_Part_1','Q39_Part_2']):\n",
    "            less_001.append(col)\n",
    "print(\"We drop nan rows in the below columns:\",less_001)\n",
    "\n",
    "for col in less_001:\n",
    "    Salary.dropna(subset=[col],inplace=True)\n",
    "print(Salary.shape)\n",
    "between_001_020 = []\n",
    "for col in Salary.columns:\n",
    "    NullNum = Salary[col].isnull().sum()\n",
    "    if (0.2 * Salary.shape[0] > NullNum) and (NullNum >= 0.01 * Salary.shape[0]):\n",
    "        if (\"Part\" not in col) or (col in ['Q39_Part_1','Q39_Part_2']):\n",
    "            between_001_020.append(col)\n",
    "print(\"We use modes in these columns to replace nan value:\",between_001_020)\n",
    "for col in between_001_020:\n",
    "    Salary[col].fillna(Salary[col].mode()[0],inplace=True)\n",
    "\n",
    "between_020_050 = []\n",
    "for col in Salary.columns:\n",
    "    NullNum = Salary[col].isnull().sum()\n",
    "    if (0.2 * Salary.shape[0]) < NullNum and (NullNum < 0.5 * Salary.shape[0]):\n",
    "        if (\"Part\" not in col) or (col in ['Q39_Part_1','Q39_Part_2']):\n",
    "            between_020_050.append(col)\n",
    "print(\"We set nans in these columns as 'Unknown':\",between_020_050)\n",
    "\n",
    "for col in between_020_050:\n",
    "    Salary.loc[Salary[col].isnull(),col] = 'Unknown'\n",
    "more_050 = []\n",
    "for col in Salary.columns:\n",
    "    NullNum = Salary[col].isnull().sum()\n",
    "    if 0.5 * Salary.shape[0] < NullNum:\n",
    "        if (\"Part\" not in col) or (col in ['Q39_Part_1','Q39_Part_2']):\n",
    "            more_050.append(col)\n",
    "print(\"We drop these columns because large part of the data is missing:\",more_050)\n",
    "\n",
    "for col in more_050:\n",
    "    Salary.drop([col], axis=1,inplace=True)\n",
    "ALLCOL = Salary.columns.tolist()\n",
    "def select_col(condition, allcol=ALLCOL):\n",
    "    digit = len(condition)\n",
    "    selected = []\n",
    "    for i in allcol:\n",
    "        if condition in i:\n",
    "            if len(i)==digit:\n",
    "                selected.append(i)\n",
    "            elif i[digit] not in '0123456789':\n",
    "                selected.append(i)                \n",
    "    return selected\n",
    "DummyCols = ['Q1','Q4','Q5','Q6','Q7','Q10','Q12_MULTIPLE_CHOICE',\\\n",
    "            'Q17','Q18','Q20','Q23','Q26','Q32','Q39_Part_1', 'Q39_Part_2',\\\n",
    "             'Q40','Q48']\n",
    "for col in DummyCols:\n",
    "    Salary = pd.get_dummies(data=Salary, columns=[col])\n",
    "# print(Salary['Q3'].value_counts())\n",
    "val_counts = Salary['Q3'].value_counts()\n",
    "value_mask = Salary['Q3'].isin(val_counts.index[val_counts < 50])\n",
    "Salary.loc[value_mask,'Q3'] = \"Other\"\n",
    "\n",
    "Salary = pd.get_dummies(data=Salary, columns=['Q3'])\n",
    "# print(Salary['Q22'].value_counts())\n",
    "val_counts = Salary['Q22'].value_counts()\n",
    "value_mask = Salary['Q22'].isin(val_counts.index[val_counts < 100])\n",
    "Salary.loc[value_mask,'Q22'] = \"Other\"\n",
    "\n",
    "Salary = pd.get_dummies(data=Salary, columns=['Q22'])\n",
    "def Range_Normalize(value):\n",
    "    if isinstance(value, str):\n",
    "        if '-' in value:\n",
    "            temp = value.split('-')\n",
    "            return (float(temp[0])+float(temp[1]))/2\n",
    "        if '+' in value:\n",
    "            a = value.split('+')\n",
    "            return float(a[0])\n",
    "        else:\n",
    "            return value\n",
    "    else:\n",
    "        return value\n",
    "# Salary[select_col('Q2')]\n",
    "Salary['Q2_NORMAL'] = Salary['Q2'].apply(Range_Normalize)\n",
    "Salary.drop(['Q2'],axis=1,inplace=True)\n",
    "# Salary[select_col('Q8')]\n",
    "Salary['Q8_NORMAL'] = Salary['Q8'].apply(Range_Normalize)\n",
    "Salary.drop(['Q8'],axis=1,inplace=True)\n",
    "# Salary[select_col('Q24')]\n",
    "def Q24Range_Normalize(value):\n",
    "    mapdict = {'I have never written code and I do not want to learn':-1,\n",
    "               'I have never written code but I want to learn':0,'< 1 year':0.5,\n",
    "              '1-2 years':1.5,'3-5 years':4,'5-10 years':7.5,'10-20 years':15,\n",
    "               '20-30 years':25,'30-40 years':35,'40+ years':40}\n",
    "    return mapdict[value]\n",
    "# Salary[select_col('Q2')]\n",
    "Salary['Q24_NORMAL'] = Salary['Q24'].apply(Q24Range_Normalize)\n",
    "Salary.drop(['Q24'],axis=1,inplace=True)\n",
    "# Salary[select_col('Q25')]\n",
    "Salary['Q25'].unique()\n",
    "def Q25Range_Normalize(value):\n",
    "    mapdict = {'I have never studied machine learning and I do not plan to':-1,\n",
    "               'I have never studied machine learning but plan to learn in the future':0,\n",
    "               '< 1 year':0.5,'1-2 years':1.5,'2-3 years':2.5,'3-4 years':3.5,'4-5 years':4.5,\n",
    "               '5-10 years':7.5,'10-15 years':12.5,'20+ years':20}\n",
    "    return mapdict[value]\n",
    "\n",
    "Salary['Q25_NORMAL'] = Salary['Q25'].apply(Q25Range_Normalize)\n",
    "Salary.drop(['Q25'],axis=1,inplace=True)\n",
    "def Q41Range_Normalize(value):\n",
    "    mapdict = {'Very important':10,\n",
    "               'Slightly important':5,\n",
    "               'No opinion; I do not know':2,\n",
    "               'Unknown':0,\n",
    "               'Not at all important':-10}\n",
    "    return mapdict[value]\n",
    "\n",
    "Salary.loc[Salary['Q41_Part_1'].isnull(),'Q41_Part_1'] = 'Unknown'\n",
    "Salary.loc[Salary['Q41_Part_2'].isnull(),'Q41_Part_2'] = 'Unknown'\n",
    "Salary.loc[Salary['Q41_Part_3'].isnull(),'Q41_Part_3'] = 'Unknown'\n",
    "\n",
    "Salary['Q41_Part_1_NORMAL'] = Salary['Q41_Part_1'].apply(Q41Range_Normalize)\n",
    "Salary.drop(['Q41_Part_1'],axis=1,inplace=True)\n",
    "Salary['Q41_Part_2_NORMAL'] = Salary['Q41_Part_2'].apply(Q41Range_Normalize)\n",
    "Salary.drop(['Q41_Part_2'],axis=1,inplace=True)\n",
    "Salary['Q41_Part_3_NORMAL'] = Salary['Q41_Part_3'].apply(Q41Range_Normalize)\n",
    "Salary.drop(['Q41_Part_3'],axis=1,inplace=True)\n",
    "def Q43Range_Normalize(value):\n",
    "    if isinstance(value, str):\n",
    "        if '-' in value:\n",
    "            temp = value.split('-')\n",
    "            return (float(temp[0])+float(temp[1]))/2\n",
    "        elif value == \"0\":\n",
    "            return 0\n",
    "        else:\n",
    "            return float(\"nan\")\n",
    "    else:\n",
    "        return value\n",
    "Salary['Q43_NORMAL'] = Salary['Q43'].apply(Q43Range_Normalize)\n",
    "Salary.drop(['Q43'],axis=1,inplace=True)\n",
    "Salary['Q43_NORMAL'].fillna(Salary['Q43_NORMAL'].mode()[0],inplace=True)\n",
    "\n",
    "Salary['Q46_NORMAL'] = Salary['Q46'].apply(Q43Range_Normalize)\n",
    "Salary.drop(['Q46'],axis=1,inplace=True)\n",
    "Salary['Q46_NORMAL'].fillna(Salary['Q46_NORMAL'].mode()[0],inplace=True)\n",
    "# Salary[select_col('Q11')]\n",
    "import math\n",
    "def Binarize(value):\n",
    "    if isinstance(value,str):\n",
    "        return 1\n",
    "    else:\n",
    "        if value == 0 or math.isnan(value):\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "def binarize_col(columns, data=Salary):\n",
    "    for col in columns:\n",
    "        data[col] = data[col].apply(Binarize)\n",
    "NormalCols = select_col('Q11_Part') + select_col('Q13_Part') + \\\n",
    "select_col('Q14_Part') + select_col('Q15_Part') + select_col('Q16_Part')\\\n",
    "+ select_col('Q19_Part') + select_col('Q21_Part') + select_col('Q27_Part')\\\n",
    "+ select_col('Q28_Part') + select_col('Q29_Part') + select_col('Q30_Part') +\\\n",
    "select_col('Q31_Part') + select_col('Q33_Part') + select_col('Q34_Part') +\\\n",
    "select_col('Q36_Part') + select_col('Q38_Part') + select_col('Q42_Part') + \\\n",
    "select_col('Q44_Part') + select_col('Q45_Part') + select_col('Q47_Part') + \\\n",
    "select_col('Q49_Part') + select_col('Q50_Part')\n",
    "binarize_col(NormalCols)\n",
    "import math\n",
    "def TBinarize(value):\n",
    "    if isinstance(value,str):\n",
    "        return float(value)\n",
    "    else:\n",
    "        if math.isnan(value):\n",
    "            return -1\n",
    "        else:\n",
    "            return value\n",
    "def Tbinarize_col(columns, data=Salary):\n",
    "    for col in columns:\n",
    "        data[col] = data[col].apply(TBinarize)\n",
    "\n",
    "Tbinarize_col(select_col('Q34_Part'))\n",
    "Tbinarize_col(select_col('Q35_Part'))\n",
    "for col in Salary.columns:\n",
    "    if isinstance(Salary.loc[1,col],str):\n",
    "        Salary[col] = Salary[col].apply(lambda x:float(x))\n",
    "Salary.rename(columns={\"Q9\": \"Yearly_compensation\"},inplace=True)\n",
    "Salary.rename(columns={\"Time from Start to Finish (seconds)\": \"TimeUse\"},inplace=True)\n",
    "Salary.drop(Salary[Salary.TimeUse < 180].index,inplace=True)\n",
    "for col in Salary.columns:\n",
    "    if len(Salary[col].value_counts())==1:\n",
    "        Salary.drop([col], axis=1,inplace=True)\n",
    "Salary.drop(Salary[Salary.Yearly_compensation < 1000].index, inplace=True)\n",
    "Salary.drop(Salary[Salary.Yearly_compensation > 350000].index, inplace=True)\n",
    "X = Salary.drop(['Yearly_compensation'],axis=1)\n",
    "Y = Salary['Yearly_compensation']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X,Y,test_size=0.3, random_state=20)\n",
    "print(X.shape,X_Train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "def feature_scaling(X_Train, X_Test, method):\n",
    "    methodmap = {'z-score':StandardScaler(),'min-max':MinMaxScaler(),'robust':RobustScaler()}\n",
    "    if method not in ['z-score','min-max','robust']:\n",
    "        print(\"We don't have this kind of method for normalization. Please choose from ['z-score','min-max','robust']\")\n",
    "        return pd.DataFrame(X_Train), pd.DataFrame(X_Test)\n",
    "    else:\n",
    "        ss = methodmap[method].fit(X_Train)\n",
    "        X_Train = ss.transform(X_Train)\n",
    "        X_Test = ss.transform(X_Test)\n",
    "        return pd.DataFrame(X_Train), pd.DataFrame(X_Test)\n",
    "X_Train_All, X_Test_All = feature_scaling(X_Train,X_Test,'z-score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def feature_elimination(X_Train, X_Test, Y_Train, method='pca', para=None):\n",
    "    if method not in ['variance','pca','mul_info']:\n",
    "        print(\"We don't have this kind of method for normalization. Please choose from ['variance','pca','mul_info']\")\n",
    "        return X_Train, X_Test\n",
    "    else:\n",
    "        if method=='variance':\n",
    "            Para = para if para else (.008 * (1 - .008))\n",
    "            sel = VarianceThreshold(threshold = Para).fit(X_Train)\n",
    "            X_Train = sel.transform(X_Train)\n",
    "            X_Test = sel.transform(X_Test)\n",
    "            return pd.DataFrame(X_Train), pd.DataFrame(X_Test)\n",
    "        elif method=='mul_info':\n",
    "            Para = para if para else 200\n",
    "            sel = SelectKBest(mutual_info_classif, k=Para).fit(X_Train,Y_Train)\n",
    "            X_Train = sel.transform(X_Train)\n",
    "            X_Test = sel.transform(X_Test)\n",
    "            return pd.DataFrame(X_Train), pd.DataFrame(X_Test)\n",
    "        else:\n",
    "            Para = para if para else 200\n",
    "            sel = PCA(n_components=Para).fit(X_Train)\n",
    "            X_Train = sel.transform(X_Train)\n",
    "            X_Test = sel.transform(X_Test)\n",
    "            return pd.DataFrame(X_Train), pd.DataFrame(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train,X_Test = feature_elimination(X_Train_All,X_Test_All,Y_Train,'pca',400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias(y_predict,y):\n",
    "    y_predict = np.array(y_predict)\n",
    "    y = np.array(y)\n",
    "    avg = np.average(y_predict)\n",
    "    return np.sqrt(np.average(np.power((avg-y),2)))\n",
    "def variance(y_predict):\n",
    "    y_predict = np.array(y_predict)\n",
    "    return np.sqrt(np.var(y_predict))\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.linear_model import LinearRegression as lr\n",
    "from sklearn.linear_model import Lasso as lasso\n",
    "from sklearn.linear_model import ElasticNet as elsnet\n",
    "from sklearn.ensemble import RandomForestRegressor as rfr\n",
    "from sklearn.ensemble import GradientBoostingRegressor as gbr\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, r2_score, mean_absolute_error\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc,confusion_matrix,accuracy_score\n",
    "\n",
    "def two_score(y_true,y_pred):    \n",
    "    mean_squared_error(y_true,y_pred) #set score here and not below if using MSE in GridCV\n",
    "    score = r2_score(y_true,y_pred)\n",
    "    return score\n",
    "\n",
    "def run_kfold(model,X,y,folds=10):\n",
    "    kf = KFold(n_splits=folds)\n",
    "    kf.get_n_splits(X)\n",
    "    score_keep = []\n",
    "    trade_keep = []\n",
    "    fold = 0\n",
    "    print(\"The result with {}-fold cross-validation on training set\".format(folds))\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        fold += 1\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        clf = model.fit(X_train, y_train)\n",
    "        y_train_predict = clf.predict(X_train)\n",
    "        y_test_predict = clf.predict(X_test)\n",
    "\n",
    "        train_r2 = r2_score(y_train, y_train_predict)\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, y_train_predict))\n",
    "        train_mae = mean_absolute_error(y_test, y_test_predict)\n",
    "        \n",
    "        test_r2 = r2_score(y_test, y_test_predict)\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_predict))\n",
    "        test_mae = mean_absolute_error(y_test, y_test_predict)\n",
    "        # print(\"Fold {} R2 training score: {}, testing score: {}\".format(fold,train_r2,test_r2))\n",
    "        # print(\"RMSE training score: {}, testing score: {}\".format(train_rmse,test_rmse))\n",
    "        train_bia = bias(y_train_predict,y_train)\n",
    "        train_var = variance(y_train_predict)\n",
    "        train_total = train_bia + train_var\n",
    "        \n",
    "        test_bia = bias(y_test_predict,y_train)\n",
    "        test_var = variance(y_test_predict)\n",
    "        test_total = test_bia + test_var\n",
    "\n",
    "        score_keep.append((train_r2, test_r2,train_rmse, test_rmse,train_mae,test_mae))\n",
    "        trade_keep.append((train_bia,test_bia,train_var,test_var,train_total,test_total))\n",
    "    score_df = pd.DataFrame(score_keep, columns=[\"train_R2\",\"test_R2\",\"train_RMSE\",\"test_RMSE\",\"train_MAE\",\"test_MAE\"])\n",
    "    trade_df = pd.DataFrame(trade_keep, columns=[\"train_bia\",\"test_bia\",\"train_var\",\"test_var\",\"train_total\",\"test_total\"])\n",
    "    print(score_df.describe()[1:4])\n",
    "    return clf, score_df, trade_df\n",
    "\n",
    "class Regressor(object):\n",
    "    def __init__(self,train_data,train_targets):\n",
    "        super().__init__()\n",
    "        self.train_data = train_data\n",
    "        self.train_targets = train_targets\n",
    "        rfrhyperpara = {'n_estimators': list(range(90,120,5)),'max_depth': list(range(40,50,3)),'min_samples_leaf': [1, 2, 3]}\n",
    "        gbrhyperpara = {'n_estimators': list(range(90,120,5)),'max_depth': list(range(4,15,3)),'min_samples_leaf': [1, 2]}\n",
    "        self.models=OrderedDict([('LinearRegression',[lr(),None]),\\\n",
    "                            ('Lasso',[lasso(alpha=9.0,max_iter=2000,random_state=20),dict(alpha=np.arange(5,15,0.5))]),\\\n",
    "                            ('ElasticNet',[elsnet(l1_ratio=0.8),dict(l1_ratio=np.linspace(0,1,10))]),\\\n",
    "                            ('RandomForest',[rfr(n_estimators = 105, max_depth=40, min_samples_leaf=2),rfrhyperpara]),\\\n",
    "                            ('GradientBoosting',[gbr(),gbrhyperpara])])\n",
    "        #1.1 and 2.70 # ElasticNet(alpha=1.0, l1_ratio=0.5）\n",
    "    def train_model(self,model_name,scorer=\"R2\",ifhyper_tune=False,hyper_paras=\"\",folder=10):\n",
    "        self.model_name = model_name\n",
    "        model = self.models[model_name][0]\n",
    "        if ifhyper_tune:\n",
    "            if not hyper_paras:\n",
    "                hyper_paras = self.models[model_name][1]\n",
    "            # Grid search method for hyper-parameter tuning in which we use 5-fold cross validation\n",
    "            if hyper_paras:\n",
    "                if scorer=='R2':\n",
    "                    acc_scorer = make_scorer(r2_score)\n",
    "                elif scorer == 'RMSE':\n",
    "                    acc_scorer = make_scorer(mean_squared_error)\n",
    "                else:\n",
    "                    acc_scorer = make_scorer(two_score, greater_is_better=True)\n",
    "                grid = GridSearchCV(model, hyper_paras, cv=5, scoring=acc_scorer, n_jobs=-1) \n",
    "\n",
    "                grid.fit(self.train_data, self.train_targets)\n",
    "                print('The best score of model {} through {}-cross validation is {}, with the best hyper-parameter {}\\n'\\\n",
    "                    .format(model_name, 5, grid.best_score_, grid.best_params_))\n",
    "\n",
    "                # Best model\n",
    "                best_model = grid.best_estimator_\n",
    "            else:\n",
    "                best_model = model.fit(self.train_data,self.train_targets)\n",
    "            train_pred = best_model.predict(self.train_data)\n",
    "            R2 = r2_score(self.train_targets,train_pred)\n",
    "            print('Through Hypertuning on {}, we get the best model and the results below:'.format(model_name))\n",
    "            print('The R2 score is {}'.format(R2))\n",
    "            RMSE = np.sqrt(mean_squared_error(self.train_targets,train_pred))\n",
    "            print('The RMSE score is {}'.format(RMSE))\n",
    "            MAE = mean_absolute_error(self.train_targets,train_pred)\n",
    "            print('The MAE score is {}'.format(MAE))\n",
    "            score_df = pd.DataFrame([(R2,RMSE,MAE)], columns=[\"R2\", \"RMSE\",\"MAE\"])\n",
    "        else:\n",
    "            best_model, score_df, trade_df = run_kfold(model,self.train_data,self.train_targets,folder)\n",
    "        \n",
    "        self.best_model = best_model\n",
    "        if ifhyper_tune:\n",
    "            return best_model, score_df\n",
    "        else:\n",
    "            return best_model, score_df, trade_df\n",
    "\n",
    "    def test_model(self,test_data,test_targets,new_best=None):\n",
    "        if new_best is not None:\n",
    "            test_pred = new_best.predict(test_data)\n",
    "        else:\n",
    "            test_pred = self.best_model.predict(test_data)\n",
    "\n",
    "        R2 = r2_score(test_targets, test_pred)\n",
    "        RMSE = np.sqrt(mean_squared_error(test_targets,test_pred))\n",
    "        MAE = mean_absolute_error(test_targets,test_pred)\n",
    "        BIAS = bias(test_pred,test_targets)\n",
    "        VAR = variance(test_pred)\n",
    "        TOTAL = BIAS + VAR\n",
    "        #test accuracy\n",
    "        print(\"Result of Model {} is list below:\".format(self.model_name))\n",
    "        print('R2 score is {}'.format(R2))\n",
    "        print('RMSE socre is {}'.format(RMSE))\n",
    "        print('MeanAbsoluteError is {}'.format(MAE))\n",
    "        print('Bias score is {}'.format(BIAS))\n",
    "        print('Variance score is {}'.format(VAR))\n",
    "        print('Bias+Variance score is {}'.format(TOTAL))\n",
    "        return R2, RMSE, MAE, BIAS, VAR, TOTAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PcaFeatureReg = Regressor(X_Train,Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result with 10-fold cross-validation on training set\n",
      "      train_R2   test_R2    train_RMSE     test_RMSE     train_MAE  \\\n",
      "mean  0.592084  0.543050  33064.841036  34953.763895  23789.538421   \n",
      "std   0.002802  0.026364    185.427076   1624.622823    639.548098   \n",
      "min   0.586728  0.503513  32741.345410  32742.397357  22813.967068   \n",
      "\n",
      "          test_MAE  \n",
      "mean  23789.538421  \n",
      "std     639.548098  \n",
      "min   22813.967068  \n"
     ]
    }
   ],
   "source": [
    "lr_model, lr_score, lr_trade = PcaFeatureReg.train_model('LinearRegression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result with 10-fold cross-validation on training set\n",
      "      train_R2   test_R2    train_RMSE     test_RMSE     train_MAE  \\\n",
      "mean  0.592072  0.543340  33065.334139  34942.598017  23772.911417   \n",
      "std   0.002802  0.026349    185.424382   1622.435139    637.484555   \n",
      "min   0.586715  0.503710  32741.844199  32735.466885  22801.940146   \n",
      "\n",
      "          test_MAE  \n",
      "mean  23772.911417  \n",
      "std     637.484555  \n",
      "min   22801.940146  \n"
     ]
    }
   ],
   "source": [
    "lasso_model, lasso_score,lasso_trade = PcaFeatureReg.train_model('Lasso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result with 10-fold cross-validation on training set\n",
      "      train_R2   test_R2    train_RMSE     test_RMSE     train_MAE  \\\n",
      "mean  0.586864  0.546145  33275.717746  34836.760560  23461.936906   \n",
      "std   0.002809  0.025860    184.489814   1644.817749    639.223899   \n",
      "min   0.581495  0.506851  32953.514032  32629.243779  22557.429044   \n",
      "\n",
      "          test_MAE  \n",
      "mean  23461.936906  \n",
      "std     639.223899  \n",
      "min   22557.429044  \n"
     ]
    }
   ],
   "source": [
    "elc_model, elc_score, elc_trade = PcaFeatureReg.train_model('ElasticNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result with 10-fold cross-validation on training set\n",
      "      train_R2   test_R2    train_RMSE     test_RMSE     train_MAE  \\\n",
      "mean  0.881849  0.320334  17794.885894  42636.247967  30313.752273   \n",
      "std   0.001398  0.022700    128.888503   1604.222084    576.410245   \n",
      "min   0.879750  0.280440  17545.751662  41312.676770  29345.907957   \n",
      "\n",
      "          test_MAE  \n",
      "mean  30313.752273  \n",
      "std     576.410245  \n",
      "min   29345.907957  \n"
     ]
    }
   ],
   "source": [
    "rfr_model, rfr_score, rfr_trade = PcaFeatureReg.train_model('RandomForest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result with 10-fold cross-validation on training set\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b1674e0db309>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgdb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgdb_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgdb_trade\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPcaFeatureReg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GradientBoosting'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-ad44c5c3bc1b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, model_name, scorer, ifhyper_tune, hyper_paras, folder)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mscore_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mRMSE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMAE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"R2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"RMSE\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"MAE\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrade_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_kfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-ad44c5c3bc1b>\u001b[0m in \u001b[0;36mrun_kfold\u001b[0;34m(model, X, y, folds)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0my_train_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0my_test_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[0;32m-> 1034\u001b[0;31m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[1;32m   1088\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m                                      X_csc, X_csr)\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m                 tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0;32m--> 788\u001b[0;31m                          check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gdb_model, gdb_score, gdb_trade = PcaFeatureReg.train_model('GradientBoosting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_score = [lr_score,lasso_score,elc_score,rfr_score,gdb_score]\n",
    "xlabels = ['LinearRegression','Lasso','ElasticNet','RandomForest','GradientBoosting']\n",
    "ylabels = all_score[0].columns\n",
    "for i in range(len(ylabels)):\n",
    "    if not i%2:\n",
    "        continue\n",
    "    score = [j.iloc[:,i] for j in all_score]\n",
    "    plt.figure()\n",
    "    plt.ylabel(ylabels[i])\n",
    "    plt.boxplot(score, labels=xlabels)\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.show()\n",
    "all_trade = [lr_trade,lasso_trade,elc_trade,rfr_trade,gdb_trade]\n",
    "ylabels = all_trade[0].columns\n",
    "for i in range(len(ylabels)):\n",
    "    if not i%2:\n",
    "        continue\n",
    "    score = [j.iloc[:,i] for j in all_trade]\n",
    "    plt.figure()\n",
    "    plt.ylabel(ylabels[i])\n",
    "    plt.boxplot(score, labels=xlabels)\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
